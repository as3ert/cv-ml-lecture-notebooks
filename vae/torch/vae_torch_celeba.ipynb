{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djI9z564ZZgD"
   },
   "source": [
    "# Convolutional Variational Autoencoders using PyTorch\n",
    "\n",
    "Markus Enzweiler, markus.enzweiler@hs-esslingen.de\n",
    "\n",
    "This is a demo used in a Computer Vision & Machine Learning lecture. Feel free to use and contribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWhhr5ouZZgG"
   },
   "source": [
    "**See `vae_torch_mnist.ipynb` for a more in-depth notebook on variational autoencoders.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E70btZtYZZgG"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Adapt `packagePath` to point to the directory containing this notebeook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvArrIPJZZgH"
   },
   "outputs": [],
   "source": [
    "# Notebook id\n",
    "nb_id = \"vae/torch\"\n",
    "\n",
    "# Imports\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bg-7y3C3ZZgI"
   },
   "outputs": [],
   "source": [
    "# Package Path (folder of this notebook)\n",
    "\n",
    "#####################\n",
    "# Local environment #\n",
    "#####################\n",
    "\n",
    "package_path = os.path.abspath(\"./\")\n",
    "\n",
    "\n",
    "#########\n",
    "# Colab #\n",
    "#########\n",
    "\n",
    "\n",
    "def check_for_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "\n",
    "# running on Colab?\n",
    "on_colab = check_for_colab()\n",
    "\n",
    "if on_colab:\n",
    "    # assume this notebook is run from Google Drive and the whole\n",
    "    # cv-ml-lecture-notebooks repo has been setup via setupOnColab.ipynb\n",
    "\n",
    "    # Google Drive mount point\n",
    "    gdrive_mnt = \"/content/drive\"\n",
    "\n",
    "    ##########################################################################\n",
    "    # Ensure that this is the same as gdrive_repo_root in setupOnColab.ipynb #\n",
    "    ##########################################################################\n",
    "    # Path on Google Drive to the cv-ml-lecture-notebooks repo\n",
    "    gdrive_repo_root = f\"{gdrive_mnt}/MyDrive/cv-ml-lecture-notebooks\"\n",
    "\n",
    "    # mount drive\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(gdrive_mnt, force_remount=True)\n",
    "\n",
    "    # set package path\n",
    "    package_path = f\"{gdrive_repo_root}/{nb_id}\"\n",
    "\n",
    "# check whether package path exists\n",
    "if not os.path.isdir(package_path):\n",
    "    raise FileNotFoundError(f\"Package path does not exist: {package_path}\")\n",
    "\n",
    "print(f\"Package path: {package_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2kXwImaZZgI"
   },
   "outputs": [],
   "source": [
    "# Additional imports\n",
    "\n",
    "# Repository Root\n",
    "repo_root = os.path.abspath(os.path.join(package_path, \"..\", \"..\"))\n",
    "# Add the repository root to the system path\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.append(repo_root)\n",
    "\n",
    "# Package Imports\n",
    "from nbutils import requirements as nb_reqs\n",
    "from nbutils import colab as nb_colab\n",
    "from nbutils import git as nb_git\n",
    "from nbutils import exec as nb_exec\n",
    "from nbutils import data as nb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHjHq68vZZgI"
   },
   "outputs": [],
   "source": [
    "# Clone git repository\n",
    "\n",
    "# Absolute path of the repository directory\n",
    "repo_dir = os.path.join(package_path, \"torch-vae\")\n",
    "repo_url = \"https://github.com/menzHSE/torch-vae.git\"\n",
    "\n",
    "nb_git.clone(repo_url, repo_dir, on_colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LhO25t8ZZgI"
   },
   "outputs": [],
   "source": [
    "# Install requirements in the current Jupyter kernel\n",
    "req_file = os.path.join(repo_dir, \"requirements.txt\")\n",
    "nb_reqs.pip_install_reqs(req_file, on_colab)\n",
    "\n",
    "# Additional requirements for this notebook\n",
    "req_file = os.path.join(package_path, \"requirements.txt\")\n",
    "nb_reqs.pip_install_reqs(req_file, on_colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCF7igNgZZgI"
   },
   "source": [
    "# Reconstruct and sample CelebA faces\n",
    "\n",
    "If the dataset cannot be automatically downloaded by PyTorch due to **daily quota exceeded** you can manually download it and put it in the ```data/celaba``` folder, see code cell below.\n",
    "\n",
    "The following files are necessary:\n",
    "- img_align_celeba.zip\n",
    "- list_attr_celeba.txt\n",
    "- list_bbox_celeba.txt\n",
    "- list_eval_partition.txt\n",
    "- list_landmarks_align_celeba.txt\n",
    "- list_landmarks_celeba.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_Dk-jekZZgJ"
   },
   "outputs": [],
   "source": [
    "# To get around the problem of \"Quota Exceeded\" on the \"official\" CelebA\n",
    "# download via torchvision (https://github.com/pytorch/vision/issues/1920),\n",
    "# we use an alternative data source.\n",
    "\n",
    "# Set this to True to download the dataset from an alternative source\n",
    "use_alternative_data_source = True\n",
    "\n",
    "if use_alternative_data_source:\n",
    "    print(\"Dowloading CelebA ... this will take a while\")\n",
    "    nb_data.download_celeba(\"./data/celeba\", on_colab, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8XU3omFZZgJ"
   },
   "source": [
    "## Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LG2xmE_UZZgJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# random seed\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Add the directory containing models.py to the system path\n",
    "sys.path.append(os.path.join(package_path, \"torch-vae\"))\n",
    "\n",
    "\n",
    "# Now we can import the model and dataset\n",
    "import model\n",
    "import dataset\n",
    "import device\n",
    "\n",
    "# parameters\n",
    "dataset_id = \"celeb-a\"\n",
    "num_latent_dims = 64\n",
    "max_num_filters = 128\n",
    "img_size = (64, 64)\n",
    "batch_size = 32\n",
    "model_id = f\"vae_filters_{max_num_filters:04d}_dims_{num_latent_dims:04d}.pth\"\n",
    "vae_fname = os.path.join(package_path, \"torch-vae\", \"models\", dataset_id, model_id)\n",
    "device = device.autoselectDevice()\n",
    "\n",
    "# load dataset\n",
    "(\n",
    "    celeba_train_loader,\n",
    "    celeba_test_loader,\n",
    "    _,\n",
    "    celeba_num_img_channels,\n",
    ") = dataset.get_loaders(dataset_id, img_size=img_size, batch_size=batch_size)\n",
    "\n",
    "# load the VAE model\n",
    "vae_celeb = model.VAE(num_latent_dims, celeba_num_img_channels, max_num_filters, device)\n",
    "vae_celeb.load_state_dict(torch.load(vae_fname, map_location=device))\n",
    "\n",
    "if vae_celeb:\n",
    "    print(f\"Model {vae_fname} loaded successfully!\")\n",
    "    print(f\"Device used: {device}\")\n",
    "    vae_celeb.to(device)\n",
    "    vae_celeb.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUML2NyoZZgJ"
   },
   "source": [
    "## Reconstruction of the CelebA test samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2irE1dDZZgJ"
   },
   "outputs": [],
   "source": [
    "def reconstructAndPlot(vae, num_latent_dims, data_loader):\n",
    "    # Take the first batch from the data_loader\n",
    "    data = next(iter(data_loader))\n",
    "    with torch.no_grad():\n",
    "        # Get the testing data and push the data to the device we are using\n",
    "        images = data[0].to(device)\n",
    "\n",
    "        # Reconstruct (encode and decode) the images\n",
    "        images_recon = vae(images)\n",
    "\n",
    "        # Interleave original and reconstructed images\n",
    "        images_comparison = torch.stack([images, images_recon], dim=1).view(\n",
    "            -1, *images.size()[1:]\n",
    "        )\n",
    "\n",
    "        # Display the images in a grid\n",
    "        # nrow is set to 2 since we want each pair (original and reconstructed) to be side by side\n",
    "        grid_img = torchvision.utils.make_grid(\n",
    "            images_comparison.cpu(), nrow=batch_size // 4\n",
    "        )\n",
    "\n",
    "    # Convert grid to numpy and transpose axes for plotting\n",
    "    grid_np = grid_img.numpy()\n",
    "    grid_np = np.transpose(grid_np, (1, 2, 0))\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(grid_np)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\n",
    "        f\"Original and Reconstructed Images with {num_latent_dims} Latent Dimensions\"\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRrIcXoQZZgK"
   },
   "outputs": [],
   "source": [
    "reconstructAndPlot(vae_celeb, num_latent_dims, celeba_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6iH5a13ZZgK"
   },
   "source": [
    "## Generate random CelebA-like samples from the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJpuOISEZZgK"
   },
   "outputs": [],
   "source": [
    "def sampleAndPlot(vae, num_latent_dims, num_samples=batch_size):\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            # generate a random latent vector\n",
    "\n",
    "            # during training we have made sure that the distribution in latent\n",
    "            # space remains close to a normal distribution\n",
    "\n",
    "            z = torch.randn(num_latent_dims).to(device)\n",
    "\n",
    "            # generate an image from the latent vector\n",
    "            img = vae.decode(z)\n",
    "\n",
    "            if i == 0:\n",
    "                pics = img\n",
    "            else:\n",
    "                pics = torch.cat((pics, img), dim=0)\n",
    "\n",
    "        # Create a grid of images\n",
    "        grid_img = torchvision.utils.make_grid(pics, nrow=batch_size // 4)\n",
    "\n",
    "        # Convert grid to numpy and transpose axes for plotting\n",
    "        grid_np = grid_img.cpu().numpy()\n",
    "        grid_np = np.transpose(grid_np, (1, 2, 0))\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(grid_np)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\n",
    "            f\"Randomly Generated Images from the VAE with {num_latent_dims} Latent Dimensions\"\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmUcE_fyZZgK"
   },
   "outputs": [],
   "source": [
    "sampleAndPlot(vae_celeb, num_latent_dims)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cv-ml-torch",
   "language": "python",
   "name": "cv-ml-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
