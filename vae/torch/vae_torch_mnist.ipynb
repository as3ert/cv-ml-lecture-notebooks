{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Variational Autoencoders using PyTorch\n",
    "\n",
    "Markus Enzweiler, markus.enzweiler@hs-esslingen.de\n",
    "\n",
    "This is a demo used in a Computer Vision & Machine Learning lecture. Feel free to use and contribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze convolutional variational autoencoders (VAEs) on datasets such as MNIST, Fashion MNIST and CelebA. We use the Python code and pretrained models from https://github.com/menzHSE/torch-vae. This notebook does not show how to train VAEs. Plese refer to https://github.com/menzHSE/torch-vae for that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good overviews of variational autoencoders are provided in [arXiv:1906.02691](https://arxiv.org/abs/1906.02691) and [arXiv:1312.6114](https://arxiv.org/abs/1312.6114).\n",
    "\n",
    "In our implementation, the input image is not directly mapped to a single latent vector. Instead, it's transformed into a probability distribution within the latent space, from which we sample a latent vector for reconstruction. The process involves:\n",
    "\n",
    "1. **Encoding to Probability Distribution**: \n",
    "   - The input image is linearly mapped to two vectors: \n",
    "     - A **mean vector**.\n",
    "     - A **standard deviation vector**.\n",
    "   - These vectors define a normal distribution in the latent space.\n",
    "\n",
    "2. **Auxiliary Loss for Distribution Shape**: \n",
    "   - We ensure the latent space distribution resembles a zero-mean unit-variance Gaussian distribution (standard normal distribution).\n",
    "   - An auxiliary loss, the Kullback-Leibler (KL) divergence between the mapped distribution and the standard normal distribution, is used in addition to the standard reconstruction loss\n",
    "   - This loss guides the training to shape the latent distribution accordingly.\n",
    "   - It ensures a well-structured and generalizable latent space for generating new images.\n",
    "\n",
    "3. **Sampling and Decoding**: \n",
    "   - The variational approach allows for sampling from the defined distribution in the latent space.\n",
    "   - These samples are then used by the decoder to generate new images.\n",
    "\n",
    "4. **Reparametrization Trick**:\n",
    "   - This trick enables backpropagation through random sampling, a crucial step in VAEs. Normally, backpropagating through a random sampling process from a distribution with mean ```mu``` and standard deviation ```sigma``` is challenging due to its nondeterministic nature.\n",
    "   - The solution involves initially sampling random values from a standard normal distribution (mean 0, standard deviation 1). These values are then linearly transformed by multiplying with ```sigma``` and adding ```mu```. This process essentially samples from our target distribution with mean ```mu``` and standard deviation ```sigma```.\n",
    "   - The key benefit of this approach is that the randomness (initial standard normal sampling) is separated from the learnable parameters (```mu``` and ```sigma```). ```Mu``` and ```sigma``` are deterministic and differentiable, allowing gradients with respect to them to be calculated during backpropagation. This enables the model to effectively learn from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Adapt `packagePath` to point to the directory containing this notebeook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook id\n",
    "nb_id = \"vae/torch\"\n",
    "\n",
    "# Imports\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package path: ./\n"
     ]
    }
   ],
   "source": [
    "# Package Path (folder of this notebook)\n",
    "\n",
    "#####################\n",
    "# Local environment #\n",
    "#####################\n",
    "\n",
    "package_path = os.path.abspath(\"./\")\n",
    "\n",
    "\n",
    "#########\n",
    "# Colab #\n",
    "#########\n",
    "\n",
    "\n",
    "def check_for_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "\n",
    "# running on Colab?\n",
    "on_colab = check_for_colab()\n",
    "\n",
    "if on_colab:\n",
    "    # assume this notebook is run from Google Drive and the whole\n",
    "    # cv-ml-lecture-notebooks repo has been setup via setupOnColab.ipynb\n",
    "\n",
    "    # Google Drive mount point\n",
    "    gdrive_mnt = \"/content/drive\"\n",
    "\n",
    "    ##########################################################################\n",
    "    # Ensure that this is the same as gdrive_repo_root in setupOnColab.ipynb #\n",
    "    ##########################################################################\n",
    "    # Path on Google Drive to the cv-ml-lecture-notebooks repo\n",
    "    gdrive_repo_root = f\"{gdrive_mnt}/MyDrive/cv-ml-lecture-notebooks\"\n",
    "\n",
    "    # mount drive\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(gdrive_mnt, force_remount=True)\n",
    "\n",
    "    # set package path\n",
    "    package_path = f\"{gdrive_repo_root}/{nb_id}\"\n",
    "\n",
    "# check whether package path exists\n",
    "if not os.path.isdir(package_path):\n",
    "    raise FileNotFoundError(f\"Package path does not exist: {package_path}\")\n",
    "\n",
    "print(f\"Package path: {package_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports\n",
    "\n",
    "# Repository Root\n",
    "repo_root = os.path.abspath(os.path.join(package_path, \"..\", \"..\"))\n",
    "# Add the repository root to the system path\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.append(repo_root)\n",
    "\n",
    "# Package Imports\n",
    "from nbutils import requirements as nb_reqs\n",
    "from nbutils import colab as nb_colab\n",
    "from nbutils import git as nb_git\n",
    "from nbutils import exec as nb_exec\n",
    "from nbutils import data as nb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository ./torch-vae exists. Resetting to HEAD...\n",
      "['git', '-C', './torch-vae', 'fetch', 'origin']\n",
      "['git', '-C', './torch-vae', 'reset', '--hard', 'origin/HEAD']\n",
      "HEAD is now at 40192f0 fixed ArgumentParser description\n",
      "Repository ./torch-vae is ready.\n"
     ]
    }
   ],
   "source": [
    "# Clone git repository\n",
    "\n",
    "# Absolute path of the repository directory\n",
    "repo_dir = os.path.join(package_path, \"torch-vae\")\n",
    "repo_url = \"https://github.com/menzHSE/torch-vae.git\"\n",
    "\n",
    "nb_git.clone(repo_url, repo_dir, on_colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/bin/python', '-m', 'pip', 'install', '-r', './torch-vae/requirements.txt']\n",
      "Requirement already satisfied: torch in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from -r ./torch-vae/requirements.txt (line 1)) (2.5.1)\n",
      "Requirement already satisfied: torchvision in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from -r ./torch-vae/requirements.txt (line 2)) (0.20.1)\n",
      "Requirement already satisfied: torchinfo in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from -r ./torch-vae/requirements.txt (line 3)) (1.8.0)\n",
      "Requirement already satisfied: numpy in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from -r ./torch-vae/requirements.txt (line 4)) (2.1.3)\n",
      "Requirement already satisfied: Pillow in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from -r ./torch-vae/requirements.txt (line 5)) (11.0.0)\n",
      "Requirement already satisfied: filelock in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from torch->-r ./torch-vae/requirements.txt (line 1)) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from torch->-r ./torch-vae/requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from torch->-r ./torch-vae/requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from torch->-r ./torch-vae/requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from torch->-r ./torch-vae/requirements.txt (line 1)) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from torch->-r ./torch-vae/requirements.txt (line 1)) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from torch->-r ./torch-vae/requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from sympy==1.13.1->torch->-r ./torch-vae/requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from jinja2->torch->-r ./torch-vae/requirements.txt (line 1)) (3.0.2)\n",
      "['/Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/bin/python', '-m', 'pip', 'install', '-r', './requirements.txt']\n",
      "Requirement already satisfied: matplotlib in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from -r ./requirements.txt (line 1)) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from matplotlib->-r ./requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from matplotlib->-r ./requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from matplotlib->-r ./requirements.txt (line 1)) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from matplotlib->-r ./requirements.txt (line 1)) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from matplotlib->-r ./requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from matplotlib->-r ./requirements.txt (line 1)) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from matplotlib->-r ./requirements.txt (line 1)) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from matplotlib->-r ./requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from matplotlib->-r ./requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->-r ./requirements.txt (line 1)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install requirements in the current Jupyter kernel\n",
    "req_file = os.path.join(repo_dir, \"requirements.txt\")\n",
    "nb_reqs.pip_install_reqs(req_file, on_colab)\n",
    "\n",
    "# Additional requirements for this notebook\n",
    "req_file = os.path.join(package_path, \"requirements.txt\")\n",
    "nb_reqs.pip_install_reqs(req_file, on_colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space Analysis using MNIST\n",
    "\n",
    "To analyze the concept of the latent space, we use a  VAE with 2 latent dimensions pretrained on MNIST from https://github.com/menzHSE/torch-vae. This makes it easy to visualize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python312.zip', '/Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12', '/Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/lib-dynload', '', '/Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages', '/Users/menz/miniconda3/envs/cv-ml-lecture-notebooks/lib/python3.12/site-packages/setuptools/_vendor', '/Users/menz/Development/cv-ml-lecture-notebooks', '/var/folders/v0/f_rgpdxj6hv48zcbtygcxd480000gn/T/tmp5pb044ju', './torch-vae', './torch-vae', './torch-vae', './torch-vae', './torch-vae', './torch-vae', './torch-vae', './torch-vae', './torch-vae']\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(sys\u001b[38;5;241m.\u001b[39mpath)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Now we can import the model and dataset\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdevice\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "# random seed\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Add the directory containing models.py to the system path\n",
    "sys.path.append(os.path.join(package_path, \"torch-vae\"))\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "\n",
    "# Now we can import the model and dataset\n",
    "import model\n",
    "import dataset\n",
    "import device\n",
    "\n",
    "# parameters\n",
    "dataset_id = \"mnist\"\n",
    "num_latent_dims = 2\n",
    "max_num_filters = 128\n",
    "img_size = (64, 64)\n",
    "batch_size = 32\n",
    "model_id = f\"vae_filters_{max_num_filters:04d}_dims_{num_latent_dims:04d}.pth\"\n",
    "vae_fname = os.path.join(package_path, \"torch-vae\", \"models\", dataset_id, model_id)\n",
    "device = device.autoselectDevice()\n",
    "\n",
    "# load dataset\n",
    "(\n",
    "    mnist_train_loader,\n",
    "    mnist_test_loader,\n",
    "    mnist_classes_list,\n",
    "    mnist_num_img_channels,\n",
    ") = dataset.get_loaders(dataset_id, img_size=img_size, batch_size=batch_size)\n",
    "\n",
    "# load the VAE model\n",
    "vae_mnist_2 = model.VAE(\n",
    "    num_latent_dims, mnist_num_img_channels, max_num_filters, device\n",
    ")\n",
    "vae_mnist_2.load_state_dict(torch.load(vae_fname, map_location=device))\n",
    "\n",
    "if vae_mnist_2:\n",
    "    print(f\"Model {vae_fname} loaded successfully!\")\n",
    "    print(f\"Device used: {device}\")\n",
    "    vae_mnist_2.to(device)\n",
    "    vae_mnist_2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show some training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get a batch of images from the training set and display them\n",
    "# we use the torchvision.utils.make_grid function to create a grid of images\n",
    "images, labels = next(iter(mnist_train_loader))\n",
    "grid_img = torchvision.utils.make_grid(images, nrow=batch_size // 4)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(np.transpose(grid_img, (1, 2, 0)))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Batch from the Training Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data distribution in latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes are clustered quite well with similar classes, such as 1 and 7, being close to each other in latent space. Classes 3,5,8 are rather mixed, since they share many common attributes. The whole distribution resembles a 2D zero-mean unit-variance Gaussian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all training images and plot the two-dimensional latent space\n",
    "# representation colored by the class label\n",
    "\n",
    "# Initialize lists to collect latent vectors and labels\n",
    "latent_vectors = []\n",
    "all_labels = []\n",
    "\n",
    "# Loop through the dataset\n",
    "for i, data in enumerate(mnist_train_loader):\n",
    "    with torch.no_grad():\n",
    "        print(f\"Encoded batch {i+1}/{len(mnist_train_loader)}\", end=\"\\r\")\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # Encode image(s) to latent vector(s)\n",
    "        z = vae_mnist_2.encode(images).cpu().numpy()\n",
    "        latent_vectors.append(z)\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "# Concatenate all collected vectors and labels\n",
    "latent_vectors = np.concatenate(latent_vectors, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Scatter plot of latent vectors\n",
    "# Adjust size (s) and color (c) as needed\n",
    "plt.scatter(\n",
    "    latent_vectors[:, 0],\n",
    "    latent_vectors[:, 1],\n",
    "    alpha=0.7,\n",
    "    c=all_labels,\n",
    "    cmap=\"tab10\",\n",
    "    s=10,\n",
    ")\n",
    "\n",
    "# Colorbar and labels\n",
    "plt.colorbar()\n",
    "plt.axis(\"equal\")\n",
    "plt.xlabel(\"Latent Dimension 1\")\n",
    "plt.ylabel(\"Latent Dimension 2\")\n",
    "plt.title(\"Latent Space Representation of MNIST Training Set\")\n",
    "\n",
    "# Adjust plot limits if needed\n",
    "xlim = [-4, 4]\n",
    "ylim = [-4, 4]\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# mean and covariance of the latent space\n",
    "mu = latent_vectors.mean(axis=0)\n",
    "cov = np.cov(latent_vectors.T)\n",
    "print(f\"Mean and covariance of the latent space:\\nmu={mu}\\ncov={cov}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize reconstructions in the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a grid of images by uniformly sampling the latent space\n",
    "# and decode the latent vectors to images\n",
    "\n",
    "# Number of images per row and column\n",
    "n = 20\n",
    "\n",
    "# Size of each image (assuming square images)\n",
    "image_size = 64\n",
    "\n",
    "# Limits of the latent space\n",
    "xlim = [-4, 4]\n",
    "ylim = [-4, 4]\n",
    "\n",
    "# Number of ticks on each axis\n",
    "num_ticks = 9\n",
    "\n",
    "# Create a grid of latent vectors\n",
    "x = np.linspace(xlim[0], xlim[1], n)\n",
    "y = np.linspace(ylim[1], ylim[0], n)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "\n",
    "# Create an empty array for the large image\n",
    "large_image = np.zeros((n * image_size, n * image_size))\n",
    "\n",
    "# Loop through the grid\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        # Get the latent vector\n",
    "        z = np.array([[xx[i, j], yy[i, j]]])\n",
    "\n",
    "        # Decode the latent vector to an image\n",
    "        with torch.no_grad():\n",
    "            x_decoded = (\n",
    "                vae_mnist_2.decode(torch.from_numpy(z).float().to(device)).cpu().numpy()\n",
    "            )\n",
    "\n",
    "        # Place the decoded image in the large array\n",
    "        large_image[\n",
    "            i * image_size : (i + 1) * image_size, j * image_size : (j + 1) * image_size\n",
    "        ] = x_decoded[0, 0]\n",
    "\n",
    "# Create a figure\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Display the large image\n",
    "plt.imshow(large_image, cmap=\"gray\")\n",
    "\n",
    "# Set the ticks to correspond to the latent space values\n",
    "tick_positions_x = np.linspace(0, n * image_size, num_ticks)\n",
    "tick_labels_x = np.linspace(xlim[0], xlim[1], num_ticks)\n",
    "plt.xticks(ticks=tick_positions_x, labels=[f\"{val:.1f}\" for val in tick_labels_x])\n",
    "\n",
    "tick_positions_y = np.linspace(0, n * image_size, num_ticks)\n",
    "tick_labels_y = np.linspace(ylim[0], ylim[1], num_ticks)\n",
    "plt.yticks(\n",
    "    ticks=tick_positions_y, labels=[f\"{val:.1f}\" for val in reversed(tick_labels_y)]\n",
    ")  # Reversed y-labels\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Latent Dimension 1\")\n",
    "plt.ylabel(\"Latent Dimension 2\")\n",
    "plt.title(\"Grid of Images Sampled from Latent Space\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction of the MNIST test samples\n",
    "\n",
    "Here, we reconstruct the (unknown) MNIST test samples by encoding and decoding them. We additionally use a model with more latent dimensions here. Two latent dimensions is good for visually analyzing the latent space but typically too few dimensions for good reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructAndPlot(vae, num_latent_dims, data_loader):\n",
    "    # Take the first batch from the data_loader\n",
    "    data = next(iter(data_loader))\n",
    "    with torch.no_grad():\n",
    "        # Get the testing data and push the data to the device we are using\n",
    "        images = data[0].to(device)\n",
    "\n",
    "        # Reconstruct (encode and decode) the images\n",
    "        images_recon = vae(images)\n",
    "\n",
    "        # Interleave original and reconstructed images\n",
    "        images_comparison = torch.stack([images, images_recon], dim=1).view(\n",
    "            -1, *images.size()[1:]\n",
    "        )\n",
    "\n",
    "        # Display the images in a grid\n",
    "        # nrow is set to 2 since we want each pair (original and reconstructed) to be side by side\n",
    "        grid_img = torchvision.utils.make_grid(\n",
    "            images_comparison.cpu(), nrow=batch_size // 4\n",
    "        )\n",
    "\n",
    "    # Convert grid to numpy and transpose axes for plotting\n",
    "    grid_np = grid_img.numpy()\n",
    "    grid_np = np.transpose(grid_np, (1, 2, 0))\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(grid_np)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\n",
    "        f\"Original and Reconstructed Images with {num_latent_dims} Latent Dimensions\"\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization using the VAE with two latent dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the VAE model\n",
    "num_latent_dims = 2\n",
    "model_id = f\"vae_filters_{max_num_filters:04d}_dims_{num_latent_dims:04d}.pth\"\n",
    "vae_fname = os.path.join(package_path, \"torch-vae\", \"models\", \"mnist\", model_id)\n",
    "\n",
    "vae_mnist2 = model.VAE(num_latent_dims, mnist_num_img_channels, max_num_filters, device)\n",
    "vae_mnist2.load_state_dict(torch.load(vae_fname, map_location=device))\n",
    "vae_mnist2.to(device)\n",
    "vae_mnist2.eval()\n",
    "\n",
    "reconstructAndPlot(vae_mnist2, num_latent_dims, mnist_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization using the VAE with 8 latent dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the VAE model\n",
    "num_latent_dims = 8\n",
    "model_id = f\"vae_filters_{max_num_filters:04d}_dims_{num_latent_dims:04d}.pth\"\n",
    "vae_fname = os.path.join(package_path, \"torch-vae\", \"models\", \"mnist\", model_id)\n",
    "\n",
    "vae_mnist_8 = model.VAE(\n",
    "    num_latent_dims, mnist_num_img_channels, max_num_filters, device\n",
    ")\n",
    "vae_mnist_8.load_state_dict(torch.load(vae_fname, map_location=device))\n",
    "vae_mnist_8.to(device)\n",
    "vae_mnist_8.eval()\n",
    "\n",
    "reconstructAndPlot(vae_mnist_8, num_latent_dims, mnist_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate random MNIST-like samples from the VAE\n",
    "\n",
    "The variational autoencoders are trained in a way that the distribution in latent space resembles a normal distribution (see above). To generate samples from the variational autoencoder, we can sample a random normally distributed latent vector and have the decoder generate an image from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleAndPlot(vae, num_latent_dims, num_samples=batch_size):\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            # generate a random latent vector\n",
    "\n",
    "            # during training we have made sure that the distribution in latent\n",
    "            # space remains close to a normal distribution\n",
    "\n",
    "            z = torch.randn(num_latent_dims).to(device)\n",
    "\n",
    "            # generate an image from the latent vector\n",
    "            img = vae.decode(z)\n",
    "\n",
    "            if i == 0:\n",
    "                pics = img\n",
    "            else:\n",
    "                pics = torch.cat((pics, img), dim=0)\n",
    "\n",
    "        # Create a grid of images\n",
    "        grid_img = torchvision.utils.make_grid(pics, nrow=batch_size // 4)\n",
    "\n",
    "        # Convert grid to numpy and transpose axes for plotting\n",
    "        grid_np = grid_img.cpu().numpy()\n",
    "        grid_np = np.transpose(grid_np, (1, 2, 0))\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(grid_np)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\n",
    "            f\"Randomly Generated Images from the VAE with {num_latent_dims} Latent Dimensions\"\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleAndPlot(vae_mnist_8, num_latent_dims=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-ml-lecture-notebooks",
   "language": "python",
   "name": "cv-ml-lecture-notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
