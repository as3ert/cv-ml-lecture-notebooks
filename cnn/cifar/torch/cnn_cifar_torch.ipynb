{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural networks (CNN) for CIFAR-10/100 using PyTorch\n",
    "\n",
    "Markus Enzweiler, markus.enzweiler@hs-esslingen.de\n",
    "\n",
    "This is a demo used in a Computer Vision & Machine Learning lecture. Feel free to use and contribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build and train a CNN for CIFAR-10 / CIFAR-100 image classification, see https://www.cs.toronto.edu/~kriz/cifar.html. We use the Python code from https://github.com/menzHSE/torch-cifar-10-cnn.git and execute it via this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Adapt `packagePath` to point to the directory containing this notebeook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "import threading\n",
    "import subprocess\n",
    "import fcntl\n",
    "import errno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package path: ./\n"
     ]
    }
   ],
   "source": [
    "# Package Path\n",
    "package_path = \"./\" # local\n",
    "print(f\"Package path: {package_path}\")\n",
    "\n",
    "\n",
    "def check_for_colab():\n",
    "  try:\n",
    "      import google.colab\n",
    "      return True\n",
    "  except ImportError:\n",
    "      return False\n",
    "\n",
    "# Running on Colab?\n",
    "on_colab = check_for_colab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository exists. Resetting to HEAD...\n",
      "HEAD is now at ba4f381 Update README.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/menzHSE/torch-cifar-10-cnn\n",
      "   e66ffc0..ba4f381  main       -> origin/main\n"
     ]
    }
   ],
   "source": [
    "# Clone git repository\n",
    "\n",
    "# Absolute path of the repository directory\n",
    "repo_dir = os.path.join(package_path, \"torch-cifar-10-cnn\")\n",
    "repo_url = \"https://github.com/menzHSE/torch-cifar-10-cnn.git\"\n",
    "\n",
    "# Store the original working directory\n",
    "original_cwd = os.getcwd()\n",
    "\n",
    "# Check if the directory already exists using the absolute path\n",
    "if os.path.exists(os.path.join(original_cwd, repo_dir)):\n",
    "    print(\"Repository exists. Resetting to HEAD...\")\n",
    "    # Navigate into the repository directory\n",
    "    os.chdir(repo_dir)\n",
    "    # Fetch the latest changes from the remote\n",
    "    subprocess.run([\"git\", \"fetch\", \"origin\"])\n",
    "    # Reset the local branch to the latest commit from the remote\n",
    "    subprocess.run([\"git\", \"reset\", \"--hard\", \"origin/HEAD\"])\n",
    "    # Change back to the original working directory\n",
    "    os.chdir(original_cwd)\n",
    "else:\n",
    "    print(\"Cloning repository...\")\n",
    "    # Clone the repository if it doesn't exist\n",
    "    subprocess.run([\"git\", \"clone\", repo_url, repo_dir])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/menzweil/Development/miniforge3/envs/pytorch-m1-2023-10/lib/python3.9/site-packages (from -r ./torch-cifar-10-cnn/requirements.txt (line 1)) (1.13.0.dev20220627)\n",
      "Requirement already satisfied: torchvision in /Users/menzweil/Development/miniforge3/envs/pytorch-m1-2023-10/lib/python3.9/site-packages (from -r ./torch-cifar-10-cnn/requirements.txt (line 2)) (0.14.0.dev20220626)\n",
      "Requirement already satisfied: torchinfo in /Users/menzweil/Development/miniforge3/envs/pytorch-m1-2023-10/lib/python3.9/site-packages (from -r ./torch-cifar-10-cnn/requirements.txt (line 3)) (1.7.2)\n",
      "Requirement already satisfied: numpy in /Users/menzweil/Development/miniforge3/envs/pytorch-m1-2023-10/lib/python3.9/site-packages (from -r ./torch-cifar-10-cnn/requirements.txt (line 4)) (1.26.0)\n",
      "Requirement already satisfied: Pillow in /Users/menzweil/Development/miniforge3/envs/pytorch-m1-2023-10/lib/python3.9/site-packages (from -r ./torch-cifar-10-cnn/requirements.txt (line 5)) (9.2.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/menzweil/Development/miniforge3/envs/pytorch-m1-2023-10/lib/python3.9/site-packages (from torch->-r ./torch-cifar-10-cnn/requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: requests in /Users/menzweil/Development/miniforge3/envs/pytorch-m1-2023-10/lib/python3.9/site-packages (from torchvision->-r ./torch-cifar-10-cnn/requirements.txt (line 2)) (2.28.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/menzweil/Development/miniforge3/envs/pytorch-m1-2023-10/lib/python3.9/site-packages (from requests->torchvision->-r ./torch-cifar-10-cnn/requirements.txt (line 2)) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/menzweil/Development/miniforge3/envs/pytorch-m1-2023-10/lib/python3.9/site-packages (from requests->torchvision->-r ./torch-cifar-10-cnn/requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/menzweil/Development/miniforge3/envs/pytorch-m1-2023-10/lib/python3.9/site-packages (from requests->torchvision->-r ./torch-cifar-10-cnn/requirements.txt (line 2)) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/menzweil/Development/miniforge3/envs/pytorch-m1-2023-10/lib/python3.9/site-packages (from requests->torchvision->-r ./torch-cifar-10-cnn/requirements.txt (line 2)) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "# Install requirements in the current Jupyter kernel\n",
    "req_file = os.path.join(repo_dir, \"requirements.txt\")\n",
    "if os.path.exists(req_file):\n",
    "    !{sys.executable} -m pip install -r {req_file}\n",
    "else:\n",
    "    print(f\"Requirements file not found: {req_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to interface with the code in the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(script_name, params=None):\n",
    "    if on_colab:\n",
    "        executeCaptureColab(script_name, params)\n",
    "    else:\n",
    "        executeCapture(script_name, params)\n",
    "\n",
    "def executeCapture(script_name, params=None):\n",
    "    script_path = os.path.join(repo_dir, script_name)\n",
    "    if os.path.exists(script_path):\n",
    "        print(f\"Executing script: {script_path}\")\n",
    "        # Create the command list starting with Python and the script path\n",
    "        command = [\"python\", script_path]\n",
    "        # Add additional arguments from the params dictionary\n",
    "        if params:\n",
    "            for key, value in params.items():\n",
    "                command.append(f\"--{key}\")\n",
    "                command.append(str(value))\n",
    "        print(command)\n",
    "        subprocess.run(command)\n",
    "    else:\n",
    "        print(f\"Script not found: {script_path}\")\n",
    "\n",
    "# This is very hacky ... but it's hard to capture the output of a subprocess in Colab\n",
    "def executeCaptureColab(script_name, params=None):\n",
    "    script_path = os.path.join(repo_dir, script_name)\n",
    "    if os.path.exists(script_path):\n",
    "        print(f\"Executing script: {script_path}\")\n",
    "        # Create the command list starting with Python and the script path\n",
    "        command = [\"python\", script_path]\n",
    "        # Add additional arguments from the params dictionary\n",
    "        if params:\n",
    "            for key, value in params.items():\n",
    "                if value is not None:  # Check if the value is None\n",
    "                    command.append(f\"--{key}\")\n",
    "                    command.append(str(value))\n",
    "                else:\n",
    "                    command.append(f\"--{key}\")\n",
    "        print(\"Command:\", \" \".join(command))\n",
    "\n",
    "        # Start the subprocess\n",
    "        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "        # Set the stdout to non-blocking\n",
    "        fd = process.stdout.fileno()\n",
    "        fl = fcntl.fcntl(fd, fcntl.F_GETFL)\n",
    "        fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)\n",
    "\n",
    "        # Function to continuously output lines from a stream\n",
    "        def stream_output(stream):\n",
    "            while True:\n",
    "                try:\n",
    "                    line = stream.readline()\n",
    "                    if line:\n",
    "                        print(line, end='')\n",
    "                    elif process.poll() is not None:\n",
    "                        break\n",
    "                except IOError as e:\n",
    "                    # Ignore the error if no data is available yet\n",
    "                    if e.errno != errno.EAGAIN and e.errno != errno.EWOULDBLOCK:\n",
    "                        raise\n",
    "\n",
    "        # Use a thread to capture the output stream\n",
    "        output_thread = threading.Thread(target=stream_output, args=(process.stdout,))\n",
    "        output_thread.start()\n",
    "\n",
    "        # Wait for the subprocess to complete and the output thread to end\n",
    "        process.wait()\n",
    "        output_thread.join()\n",
    "\n",
    "    else:\n",
    "        print(f\"Script not found: {script_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing script: ./torch-cifar-10-cnn/train.py\n",
      "['python', './torch-cifar-10-cnn/train.py', '--help', 'None']\n",
      "usage: Train a simple CNN on CIFAR-10 / CIFAR_100 with PyTorch.\n",
      "       [-h] [--cpu] [--seed SEED] [--batchsize BATCHSIZE] [--epochs EPOCHS]\n",
      "       [--lr LR] [--dataset {CIFAR-10,CIFAR-100}]\n",
      "       [--finetune {resnet18,resnet34,resnet50}]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --cpu                 Use CPU instead of GPU (cuda/mps) acceleration\n",
      "  --seed SEED           Random seed\n",
      "  --batchsize BATCHSIZE\n",
      "                        Batch size for training\n",
      "  --epochs EPOCHS       Number of training epochs\n",
      "  --lr LR               Learning rate\n",
      "  --dataset {CIFAR-10,CIFAR-100}\n",
      "                        Select the dataset to use (CIFAR-10 or CIFAR-100)\n",
      "  --finetune {resnet18,resnet34,resnet50}\n",
      "                        Select the model for fine-tuning (resnet18, resnet34,\n",
      "                        resnet50), omit for training from scratch\n"
     ]
    }
   ],
   "source": [
    "# Let's see what we can do with train.py\n",
    "execute(\"train.py\", {\"help\": None})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test CNN on CIFAR-10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "batchsize = 32\n",
    "seed      = 42\n",
    "lr        = 3e-4\n",
    "epochs    = 30\n",
    "dataset   = \"CIFAR-10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing script: ./torch-cifar-10-cnn/train.py\n",
      "['python', './torch-cifar-10-cnn/train.py', '--dataset', 'CIFAR-10', '--batchsize', '32', '--seed', '42', '--lr', '0.0003', '--epochs', '30']\n",
      "Using device: mps\n",
      "Options: \n",
      "  Device: GPU\n",
      "  Seed: 42\n",
      "  Batch size: 32\n",
      "  Number of epochs: 30\n",
      "  Learning rate: 0.0003\n",
      "  Dataset: CIFAR-10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/menzweil/Development/miniforge3/envs/pytorch-m1-2023-10/lib/python3.9/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type (var_name))                  Output Shape              Param #\n",
      "==========================================================================================\n",
      "CNN (CNN)                                [1, 10]                   --\n",
      "├─Conv2d (conv1)                         [1, 32, 32, 32]           896\n",
      "├─BatchNorm2d (bn1)                      [1, 32, 32, 32]           64\n",
      "├─Conv2d (skip2)                         [1, 32, 32, 32]           1,056\n",
      "├─Conv2d (conv2)                         [1, 32, 32, 32]           9,248\n",
      "├─BatchNorm2d (bn2)                      [1, 32, 32, 32]           64\n",
      "├─MaxPool2d (pool)                       [1, 32, 16, 16]           --\n",
      "├─Dropout (drop)                         [1, 32, 16, 16]           --\n",
      "├─Conv2d (conv3)                         [1, 64, 16, 16]           18,496\n",
      "├─BatchNorm2d (bn3)                      [1, 64, 16, 16]           128\n",
      "├─Conv2d (skip4)                         [1, 64, 16, 16]           4,160\n",
      "├─Conv2d (conv4)                         [1, 64, 16, 16]           36,928\n",
      "├─BatchNorm2d (bn4)                      [1, 64, 16, 16]           128\n",
      "├─MaxPool2d (pool)                       [1, 64, 8, 8]             --\n",
      "├─Dropout (drop)                         [1, 64, 8, 8]             --\n",
      "├─Conv2d (conv5)                         [1, 128, 8, 8]            73,856\n",
      "├─BatchNorm2d (bn5)                      [1, 128, 8, 8]            256\n",
      "├─Conv2d (skip6)                         [1, 128, 8, 8]            16,512\n",
      "├─Conv2d (conv6)                         [1, 128, 8, 8]            147,584\n",
      "├─BatchNorm2d (bn6)                      [1, 128, 8, 8]            256\n",
      "├─MaxPool2d (pool)                       [1, 128, 4, 4]            --\n",
      "├─Dropout (drop)                         [1, 128, 4, 4]            --\n",
      "├─Linear (fc1)                           [1, 128]                  262,272\n",
      "├��Linear (fc2)                           [1, 10]                   1,290\n",
      "==========================================================================================\n",
      "Total params: 573,194\n",
      "Trainable params: 573,194\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 42.22\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.29\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 4.60\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch   0] :  ........"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m: dataset,           \u001b[38;5;66;03m# dataset name\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatchsize\u001b[39m\u001b[38;5;124m\"\u001b[39m: batchsize,       \u001b[38;5;66;03m# batch size\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: epochs              \u001b[38;5;66;03m# number of epochs     \u001b[39;00m\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Execute 'train.py' with parameters\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(script_name, params)\u001b[0m\n\u001b[1;32m      3\u001b[0m     executeCaptureColab(script_name, params)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mexecuteCapture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m, in \u001b[0;36mexecuteCapture\u001b[0;34m(script_name, params)\u001b[0m\n\u001b[1;32m     17\u001b[0m             command\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(value))\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(command)\n\u001b[0;32m---> 19\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScript not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscript_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Development/miniforge3/envs/pytorch-m1-2023-10/lib/python3.9/subprocess.py:507\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 507\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    509\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/Development/miniforge3/envs/pytorch-m1-2023-10/lib/python3.9/subprocess.py:1126\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1124\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1126\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Development/miniforge3/envs/pytorch-m1-2023-10/lib/python3.9/subprocess.py:1189\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Development/miniforge3/envs/pytorch-m1-2023-10/lib/python3.9/subprocess.py:1933\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 1933\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   1935\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   1936\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m~/Development/miniforge3/envs/pytorch-m1-2023-10/lib/python3.9/subprocess.py:1891\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1891\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1892\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   1893\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   1894\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   1896\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"dataset\": dataset,           # dataset name\n",
    "    \"batchsize\": batchsize,       # batch size\n",
    "    \"seed\": seed,                 # random seed\n",
    "    \"lr\": lr,                     # learning rate\n",
    "    \"epochs\": epochs              # number of epochs     \n",
    "}\n",
    "\n",
    "# Execute 'train.py' with parameters\n",
    "execute(\"train.py\", params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "params = {\n",
    "    \"model\": f\"models/model_{dataset}_{epochs-1:03d}.pth\" # model name    \n",
    "}\n",
    "\n",
    "# Execute 'train.py' with parameters\n",
    "execute(\"test.py\", params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test CNN on CIFAR-100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "batchsize = 32\n",
    "seed      = 42\n",
    "lr        = 3e-4\n",
    "epochs    = 30\n",
    "dataset   = \"CIFAR-100\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"dataset\": dataset,           # dataset name\n",
    "    \"batchsize\": batchsize,       # batch size\n",
    "    \"seed\": seed,                 # random seed\n",
    "    \"lr\": lr,                     # learning rate\n",
    "    \"epochs\": epochs              # number of epochs\n",
    "}\n",
    "\n",
    "# Execute 'train.py' with parameters\n",
    "execute(\"train.py\", params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "params = {\n",
    "    \"dataset\": dataset,           # dataset name\n",
    "    \"model\": f\"models/model_{dataset}_{epochs-1:03d}.pth\" # model name    \n",
    "}\n",
    "\n",
    "# Execute 'train.py' with parameters\n",
    "execute(\"test.py\", params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune a ResNet on CIFAR-100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "batchsize = 32\n",
    "seed      = 42\n",
    "lr        = 3e-4\n",
    "epochs    = 5\n",
    "dataset   = \"CIFAR-100\"\n",
    "finetune  = \"resnet18\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing script: ./torch-cifar-10-cnn/train.py\n",
      "['python', './torch-cifar-10-cnn/train.py', '--dataset', 'CIFAR-100', '--batchsize', '32', '--seed', '42', '--lr', '0.0003', '--epochs', '5', '--finetune', 'resnet18']\n",
      "Using device: mps\n",
      "Options: \n",
      "  Device: GPU\n",
      "  Seed: 42\n",
      "  Batch size: 32\n",
      "  Number of epochs: 5\n",
      "  Learning rate: 0.0003\n",
      "  Dataset: CIFAR-100\n",
      "  Fine-tuning model: resnet18\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 32768/169001437 [00:00<20:46, 135567.98it/s]/Users/menzweil/Development/miniforge3/envs/pytorch-m1-2023-10/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 20 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "100%|██████████| 169001437/169001437 [02:57<00:00, 950974.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/menzweil/Development/miniforge3/envs/pytorch-m1-2023-10/lib/python3.9/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type (var_name))                       Output Shape              Param #\n",
      "===============================================================================================\n",
      "CNNResnet (CNNResnet)                         [1, 100]                  --\n",
      "├─ResNet (backbone)                           [1, 100]                  --\n",
      "│    └─Conv2d (conv1)                         [1, 64, 112, 112]         9,408\n",
      "│    └─BatchNorm2d (bn1)                      [1, 64, 112, 112]         128\n",
      "│    └─ReLU (relu)                            [1, 64, 112, 112]         --\n",
      "│    └─MaxPool2d (maxpool)                    [1, 64, 56, 56]           --\n",
      "│    └─Sequential (layer1)                    [1, 64, 56, 56]           --\n",
      "│    │    └─BasicBlock (0)                    [1, 64, 56, 56]           73,984\n",
      "│    │    └─BasicBlock (1)                    [1, 64, 56, 56]           73,984\n",
      "│    └─Sequential (layer2)                    [1, 128, 28, 28]          --\n",
      "│    │    └─BasicBlock (0)                    [1, 128, 28, 28]          230,144\n",
      "│    │    └─BasicBlock (1)                    [1, 128, 28, 28]          295,424\n",
      "│    └─Sequential (layer3)                    [1, 256, 14, 14]          --\n",
      "│    │    └─BasicBlock (0)                    [1, 256, 14, 14]          919,040\n",
      "│    │    └─BasicBlock (1)                    [1, 256, 14, 14]          1,180,672\n",
      "│    └─Sequential (layer4)                    [1, 512, 7, 7]            --\n",
      "│    │    └─BasicBlock (0)                    [1, 512, 7, 7]            3,673,088\n",
      "│    │    └─BasicBlock (1)                    [1, 512, 7, 7]            4,720,640\n",
      "│    └─AdaptiveAvgPool2d (avgpool)            [1, 512, 1, 1]            --\n",
      "│    └─Linear (fc)                            [1, 100]                  51,300\n",
      "===============================================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 1.81\n",
      "===============================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 39.74\n",
      "Params size (MB): 44.91\n",
      "Estimated Total Size (MB): 85.25\n",
      "===============================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch   0] :  ................ done (1563 batches)\n",
      "[Epoch   0] : | time: 694.138s | trainLoss:  1.676 | trainAccuracy:  0.550 | valLoss:  1.192 | valAccuracy:  0.653 | throughput:    285.911 img/s |\n",
      "[Epoch   1] :  ................ done (1563 batches)\n",
      "[Epoch   1] : | time: 684.555s | trainLoss:  0.933 | trainAccuracy:  0.725 | valLoss:  1.086 | valAccuracy:  0.688 | throughput:    289.701 img/s |\n",
      "[Epoch   2] :  ................ done (1563 batches)\n",
      "[Epoch   2] : | time: 679.002s | trainLoss:  0.636 | trainAccuracy:  0.806 | valLoss:  0.976 | valAccuracy:  0.721 | throughput:    293.090 img/s |\n",
      "[Epoch   3] :  ................ done (1563 batches)\n",
      "[Epoch   3] : | time: 683.615s | trainLoss:  0.414 | trainAccuracy:  0.872 | valLoss:  1.053 | valAccuracy:  0.711 | throughput:    291.201 img/s |\n",
      "[Epoch   4] :  ................ done (1563 batches)\n",
      "[Epoch   4] : | time: 678.860s | trainLoss:  0.298 | trainAccuracy:  0.907 | valLoss:  1.150 | valAccuracy:  0.712 | throughput:    293.938 img/s |\n",
      "Training finished in 0:57:01.107456 hh:mm:ss.ms\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"dataset\": dataset,           # dataset name\n",
    "    \"batchsize\": batchsize,       # batch size\n",
    "    \"seed\": seed,                 # random seed\n",
    "    \"lr\": lr,                     # learning rate\n",
    "    \"epochs\": epochs,             # number of epochs\n",
    "    \"finetune\": finetune          # finetune model\n",
    "}\n",
    "\n",
    "# Execute 'train.py' with parameters\n",
    "execute(\"train.py\", params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing script: ./torch-cifar-10-cnn/test.py\n",
      "['python', './torch-cifar-10-cnn/test.py', '--dataset', 'CIFAR-100', '--model', 'models/model_resnet18_CIFAR-100_004.pth', '--finetune', 'resnet18']\n",
      "Using device: mps\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "params = {\n",
    "    \"dataset\": dataset,           # dataset name\n",
    "    \"model\": f\"models/model_{finetune}_{dataset}_{epochs-1:03d}.pth\", # model name  \n",
    "    \"finetune\": finetune          # finetune model  \n",
    "}\n",
    "\n",
    "# Execute 'train.py' with parameters\n",
    "execute(\"test.py\", params=params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-m1-2023-10",
   "language": "python",
   "name": "pytorch-m1-2023-10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
