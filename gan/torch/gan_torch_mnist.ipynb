{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Deep Generative Adversarial Networks (DCGAN) using PyTorch\n",
    "\n",
    "Author: [Markus Enzweiler](https://markus-enzweiler-de), markus.enzweiler@hs-esslingen.de\n",
    "\n",
    "This is a demo used in a Computer Vision & Machine Learning lecture. Feel free to use and contribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze convolutional GANs on datasets such as MNIST and CelebA. We use the Python code and pretrained models from https://github.com/menzHSE/torch-gan. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This notebook does not show how to train GANs. Plese refer to https://github.com/menzHSE/torch-gan for that. Here, we use pretrained models from the aforementioned repository. \n",
    "\n",
    "Training a GAN involves understanding it as a two-player min-max game with two neural networks: the generator and the discriminator. The generator's goal is to produce data that mimics real data, attempting to deceive the discriminator. It learns and improves based on the discriminator's feedback. The discriminator evaluates both real and generated data, honing its ability to distinguish between the two.\n",
    "\n",
    "The core of GAN training is this competitive process. The generator aims to create increasingly realistic data, while the discriminator tries to accurately identify fakes. This competition drives both networks to improve over time. However, maintaining a balance is crucial; if one network outperforms the other significantly, it can lead to training issues. The challenge in GAN training is achieving this balance and ensuring neither network dominates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the Generator's Progress during Training\n",
    "\n",
    "### MNIST\n",
    "\n",
    "![MNIST Progress](https://github.com/menzHSE/cv-ml-lecture-notebooks/blob/main/assets/mnist.gif?raw=true)\n",
    "\n",
    "\n",
    "### CIFAR-10\n",
    "\n",
    "![CIFAR-10 Progress](https://github.com/menzHSE/cv-ml-lecture-notebooks/blob/main/assets/cifar-10.gif?raw=true)\n",
    "\n",
    "\n",
    "### CelebA\n",
    "\n",
    "![CelebA Progress](https://github.com/menzHSE/cv-ml-lecture-notebooks/blob/main/assets/celeb-a.gif?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Implementation Overview\n",
    "\n",
    "The architecture of the implemented DCGANs in the repository https://github.com/menzHSE/torch-gan follows the influential paper [\"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\"](https://arxiv.org/abs/1511.06434):\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/menzHSE/cv-ml-lecture-notebooks/blob/main/assets/generator_architecture.jpg?raw=true\" width=\"600px\" />\n",
    "<br>\n",
    "\n",
    "<sup>(Figure taken from https://arxiv.org/abs/1511.06434)</sup>\n",
    "<br>\n",
    "<br>\n",
    "Particularly, we follow the best practices and parameter settings mentioned in the paper:\n",
    "\n",
    "\n",
    "- **Fully Convolutional Architecture:** Adapts a convolutional approach throughout the model for both the generator and discriminator, enhancing feature extraction capabilities.\n",
    "- **Architecture Guidelines for Stability:** Incorporates best practices for stable training of deep convolutional GANs, such as:\n",
    "  - Replacing pooling layers with strided convolutions in the discriminator and transposed convolutions in the generator.\n",
    "  - Utilizing batch normalization in both the generator and discriminator to improve training dynamics.\n",
    "  - Omitting fully connected hidden layers in deeper architectures to streamline the network.\n",
    "  - Employing ReLU activation in the generator for all layers except the output, which uses a Tanh function.\n",
    "  - Integrating LeakyReLU activation in the discriminator for all layers, enhancing gradient flow.\n",
    "  - Custom weight initialization and Adam optimizer with adapted parameters, as given in the DCGAN paper.  \n",
    "\n",
    "For another guide on creating a DCGAN with PyTorch, including additional insights and practical tips, also check out the [PyTorch DCGAN tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Adapt `packagePath` to point to the directory containing this notebeook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports\n",
    "\n",
    "# Repository Root\n",
    "repo_root = os.path.abspath(os.path.join(\"..\", \"..\"))\n",
    "# Add the repository root to the system path\n",
    "sys.path.append(repo_root)\n",
    "\n",
    "# Package Imports\n",
    "from nbutils import requirements as nb_reqs\n",
    "from nbutils import colab as nb_clab\n",
    "from nbutils import git as nb_git\n",
    "from nbutils import exec as nb_exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Path\n",
    "package_path = \"./\" # local\n",
    "print(f\"Package path: {package_path}\")\n",
    "\n",
    "# Running on Colab?\n",
    "on_colab = nb_clab.check_for_colab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone git repository\n",
    "\n",
    "# Absolute path of the repository directory\n",
    "repo_dir = os.path.join(package_path, \"torch-gan\")\n",
    "repo_url = \"https://github.com/menzHSE/torch-gan.git\"\n",
    "\n",
    "nb_git.clone(repo_url, repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements in the current Jupyter kernel\n",
    "req_file = os.path.join(repo_dir, \"requirements.txt\")\n",
    "nb_reqs.pip_install_reqs(req_file)\n",
    "\n",
    "# Additional requirements for this notebook\n",
    "req_file = os.path.join(os.getcwd(), \"requirements.txt\")\n",
    "nb_reqs.pip_install_reqs(req_file)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space Analysis using MNIST\n",
    "\n",
    "To analyze the concept of the latent space, we use a special generator network that has been trained with 2 latent dimensions  MNIST from https://github.com/menzHSE/torch-gan. This makes it easy to visualize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# random seed\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Add the directory containing models.py to the system path\n",
    "sys.path.append(os.path.join(package_path, 'torch-gan'))\n",
    "\n",
    "\n",
    "# Now we can import the model and dataset\n",
    "import model\n",
    "import dataset\n",
    "import device\n",
    "\n",
    "# parameters\n",
    "dataset_id       = \"mnist\"\n",
    "num_latent_dims  = 2\n",
    "max_num_filters  = 512\n",
    "img_size         = (64, 64)\n",
    "batch_size       = 32\n",
    "model_id         = f\"G_filters_{max_num_filters:04d}_dims_{num_latent_dims:04d}.pth\"\n",
    "gen_fname        = os.path.join(package_path, \"torch-gan\", \"pretrained\", dataset_id, model_id)\n",
    "dev              = device.autoselectDevice()\n",
    "\n",
    "# load dataset\n",
    "# for GANs, this is normalised to [-1, 1]\n",
    "mnist_train_loader, mnist_test_loader, mnist_classes_list, mnist_num_img_channels = dataset.get_loaders(\n",
    "    dataset_id, img_size=img_size, batch_size=batch_size)\n",
    "\n",
    "# load the generator\n",
    "G_lat2 = model.Generator(num_latent_dims, mnist_num_img_channels, max_num_filters, dev)\n",
    "G_lat2.load(gen_fname, dev)\n",
    "\n",
    "if G_lat2:\n",
    "    print(f\"Model {gen_fname} loaded successfully!\")\n",
    "    print(f\"Device used: {dev}\")\n",
    "    G_lat2.to(dev)\n",
    "    G_lat2.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show some Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeForDisplay(images):\n",
    "    # normalize from [-1, 1] to [0, 1]\n",
    "    return (images + 1.0) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get a batch of images from the training set and display them\n",
    "# we use the torchvision.utils.make_grid function to create a grid of images\n",
    "images, labels = next(iter(mnist_train_loader))\n",
    "grid_img = torchvision.utils.make_grid(normalizeForDisplay(images), nrow=batch_size//4)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(np.transpose(grid_img, (1,2,0)))\n",
    "plt.axis('off')\n",
    "plt.title('Batch from the Training Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the Latent Space and Show Generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a grid of images by uniformly sampling the latent space\n",
    "# and generate images from the the latent vectors\n",
    "\n",
    "# Number of images per row and column\n",
    "n = 20\n",
    "\n",
    "# Size of each image (assuming square images)\n",
    "image_size = 64  \n",
    "\n",
    "# Limits of the latent space\n",
    "xlim = [-2, 2]\n",
    "ylim = [-2, 2]\n",
    "\n",
    "# Number of ticks on each axis\n",
    "num_ticks = 9\n",
    "\n",
    "# Create a grid of latent vectors\n",
    "x = np.linspace(xlim[0], xlim[1], n)\n",
    "y = np.linspace(ylim[1], ylim[0], n)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "\n",
    "# Create an empty array for the large image\n",
    "large_image = np.zeros((n * image_size, n * image_size))\n",
    "\n",
    "# Loop through the grid\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        # Get the latent vector\n",
    "        z = np.array([[xx[i, j], yy[i, j]]])\n",
    "        \n",
    "        # Decode the latent vector to an image\n",
    "        with torch.no_grad():\n",
    "            x_generated = G_lat2(torch.from_numpy(z).float().to(dev)).cpu().numpy()\n",
    "        \n",
    "        # Place the generated image in the large array\n",
    "        large_image[i*image_size:(i+1)*image_size, j*image_size:(j+1)*image_size] = x_generated[0, 0]\n",
    "\n",
    "# Create a figure\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Display the large image\n",
    "plt.imshow(normalizeForDisplay(large_image), cmap='gray')\n",
    "\n",
    "# Set the ticks to correspond to the latent space values\n",
    "tick_positions_x = np.linspace(0, n*image_size, num_ticks)\n",
    "tick_labels_x = np.linspace(xlim[0], xlim[1], num_ticks)\n",
    "plt.xticks(ticks=tick_positions_x, labels=[f'{val:.1f}' for val in tick_labels_x])\n",
    "\n",
    "tick_positions_y = np.linspace(0, n*image_size, num_ticks)\n",
    "tick_labels_y = np.linspace(ylim[0], ylim[1], num_ticks)\n",
    "plt.yticks(ticks=tick_positions_y, labels=[f'{val:.1f}' for val in reversed(tick_labels_y)])  # Reversed y-labels\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.title('Grid of Images Generated from Samples in the Latent Space')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Random MNIST-like Samples from the Generator\n",
    "\n",
    "The generator in a DCGAN is pivotal for creating new images. It begins with a random latent vector, typically following a normal distribution, as its input. This vector, representing a random point in latent space, is what the generator uses to produce an image.\n",
    "\n",
    "When generating an image, the generator's role is to map this latent vector to an image that mirrors the distribution of real images it's been trained on. This process involves the transformation of the input vector's distribution into something that resembles the data distribution of real images, that is however not explicity estimated. It is just sampled from with the help of the generator.  \n",
    "\n",
    "**Steps for Image Generation**\n",
    "\n",
    "- Latent vector creation: Start with a random latent vector sampled from a normal distribution. The size of this vector is defined by the GAN's architecture.\n",
    "\n",
    "- Transformation the by generator: Feed the latent vector into the generator network. The generator then uses its learned parameters to transform this vector into an image.\n",
    "\n",
    "- Output: The result is a synthetic image that, ideally, looks similar to the images the network was trained on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Generator with 100 Latent Dimensions (see DCGAN Paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "dataset_id       = \"mnist\"\n",
    "num_latent_dims  = 100\n",
    "max_num_filters  = 512\n",
    "img_size         = (64, 64)\n",
    "batch_size       = 32\n",
    "model_id         = f\"G_filters_{max_num_filters:04d}_dims_{num_latent_dims:04d}.pth\"\n",
    "gen_fname        = os.path.join(package_path, \"torch-gan\", \"pretrained\", dataset_id, model_id)\n",
    "dev              = device.autoselectDevice()\n",
    "\n",
    "\n",
    "# load the generator\n",
    "G_lat100 = model.Generator(num_latent_dims, mnist_num_img_channels, max_num_filters, dev)\n",
    "G_lat100.load(gen_fname, dev)\n",
    "\n",
    "if G_lat100:\n",
    "    print(f\"Model {gen_fname} loaded successfully!\")\n",
    "    print(f\"Device used: {dev}\")\n",
    "    G_lat100.to(dev)\n",
    "    G_lat100.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "def sampleAndPlot(G, num_latent_dims, num_samples=batch_size):\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):         \n",
    "      \n",
    "            # generate a random latent vector   \n",
    "            z = utils.sample_latent_vectors(1, num_latent_dims, dev)\n",
    "\n",
    "            # generate an image from the latent vector\n",
    "            img = G(z)\n",
    "\n",
    "            if i == 0:\n",
    "                pics = img\n",
    "            else:\n",
    "                pics = torch.cat((pics, img), dim=0)\n",
    "\n",
    "          \n",
    "        # Create a grid of images\n",
    "        grid_img = torchvision.utils.make_grid(pics, nrow=batch_size//4)\n",
    "\n",
    "        # Convert grid to numpy and transpose axes for plotting\n",
    "        grid_np = grid_img.cpu().numpy()\n",
    "        grid_np = np.transpose(grid_np, (1, 2, 0))\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(normalizeForDisplay(grid_np))\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Randomly Generated Images from the generator with {num_latent_dims} Latent Dimensions')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleAndPlot(G_lat100, num_latent_dims=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-m1-2023-10",
   "language": "python",
   "name": "pytorch-m1-2023-10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
