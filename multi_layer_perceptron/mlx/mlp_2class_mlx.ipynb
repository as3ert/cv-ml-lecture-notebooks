{"cells":[{"cell_type":"markdown","metadata":{"id":"epxcwtWj5yJs"},"source":["# Multi-Layer Perceptron for a toy two-class problem in MLX\n","\n","Markus Enzweiler, markus.enzweiler@hs-esslingen.de\n","\n","This is a demo used in a Computer Vision & Machine Learning lecture. Feel free to use and contribute."]},{"cell_type":"markdown","metadata":{},"source":["We build and train a multi-layer perceptron (MLP) for a two-class classification problem with a *single* neuron in its output layer. The MLP will output values from 0-1 and we can use a threshold of 0.5 to determine the class label.\n","\n","We will also train a single perceptron for comparison. \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","**Note: This requires a machine with an Apple SoC, e.g. M1/M2/M3 etc.**\n","\n","See: https://github.com/ml-explore/mlx"]},{"cell_type":"markdown","metadata":{"id":"IJsjl_l47q28"},"source":["## Setup\n","\n","Adapt `packagePath` to point to the directory containing this notebeook."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":236,"status":"ok","timestamp":1703326573912,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"1iHkPBml98YG"},"outputs":[],"source":["# Imports\n","import sys\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Additional imports\n","\n","# Repository Root\n","repo_root = os.path.abspath(os.path.join(\"..\", \"..\"))\n","# Add the repository root to the system path\n","sys.path.append(repo_root)\n","\n","# Package Imports\n","from nbutils import requirements as nb_reqs\n","from nbutils import colab as nb_clab\n","from nbutils import git as nb_git\n","from nbutils import exec as nb_exec"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2602,"status":"ok","timestamp":1703326576880,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"DY4880S378_F","outputId":"a8ce2d26-fe24-4aa2-c232-acc31b71d394"},"outputs":[],"source":["# Package Path\n","package_path = \"./\" # local\n","print(f\"Package path: {package_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6888,"status":"ok","timestamp":1703326583765,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"iaURjw5n6pLq","outputId":"5c40ee52-1fa3-44df-a94e-b81bec144100"},"outputs":[],"source":["# Additional requirements for this notebook\n","req_file = os.path.join(package_path, \"requirements.txt\")\n","nb_reqs.pip_install_reqs(req_file)    "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1703326583765,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"iRERDI8aAnzr"},"outputs":[],"source":["# Now we should be able to import the additional packages\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import mlx\n","import mlx.core as mx\n","import mlx.nn as nn\n","import mlx.optimizers as optim\n","\n","# Set the random seed for reproducibility\n","np.random.seed(42)\n","mx.random.seed(42)\n"]},{"cell_type":"markdown","metadata":{"id":"aUACL9QoFTXg"},"source":["## Create the training and validation data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Number of samples per class\n","n_samples = 1000\n","\n","# Generate random data for class 1\n","class_0 = mx.concatenate([\n","    mx.random.normal((n_samples//4, 2)) + mx.array([2, 0]),\n","    mx.random.normal((n_samples//2, 2)) + mx.array([0, 2]),\n","    mx.random.normal((n_samples//4, 2)) + mx.array([2, 5])\n","    ])\n","\n","\n","# Generate random data for class 2\n","class_1 = mx.random.normal((n_samples, 2)) + mx.array([4, 3])\n","\n","# Labels for the classes\n","labels_0 = mx.zeros((n_samples, 1))\n","labels_1 = mx.ones ((n_samples, 1))\n","\n","# Combine the data and labels\n","data   = mx.concatenate([class_0, class_1],   axis=0)\n","labels = mx.concatenate([labels_0, labels_1], axis=0)\n","\n","# Plotting the data\n","plt.scatter(class_0[:, 0], class_0[:, 1], color='red', label='Class 0')\n","plt.scatter(class_1[:, 0], class_1[:, 1], color='blue',  label='Class 1')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.title('Two-dimensional Dataset with Two Classes')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split the data into training and validation data sets\n","# We use NumPy and convert later into MLX arrays. Must be a better way in MLX, maybe with\n","# the upcoming mlx-data package.\n","\n","# Combine data and labels into a dataset\n","dataset = np.column_stack((data, labels))\n","\n","# Shuffle the dataset\n","np.random.shuffle(dataset)\n","\n","# Split the data and labels back from the combined dataset\n","data, labels = dataset[:, :-1], dataset[:, -1]\n","\n","# Define the size of the training and validation sets\n","train_size = int(0.8 * len(dataset))    # 80% for training\n","val_size   = len(dataset) - train_size  # 20% for validation\n","\n","# Split the dataset into training and validation sets\n","train_data   = mx.array(data[:train_size])\n","train_labels = mx.array(labels[:train_size])\n","\n","val_data   = mx.array(data[train_size:])\n","val_labels = mx.array(labels[train_size:])\n"]},{"cell_type":"markdown","metadata":{},"source":["# Define the Multi-Layer Perceptron (MLP) and a single Perceptron"]},{"cell_type":"markdown","metadata":{},"source":["## Single perceptron class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Perceptron(nn.Module):\n","    # override constructor from nn.Module\n","    def __init__(self, num_inputs):\n","        super().__init__() ## call constructor of nn.Module\n","\n","        # we define the components of our perceptron\n","        # we want to have a linear layer with num_inputs inputs and one output\n","        self.linear = nn.Linear(input_dims=num_inputs, output_dims=1)\n","        self.sigmoid = mx.sigmoid\n","\n","    def __call__(self, x):\n","        return self.forward(x)\n","\n","    def forward(self, x):\n","        # and the computation of the forward pass\n","        return self.sigmoid(self.linear(x))"]},{"cell_type":"markdown","metadata":{},"source":["## MLP class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MultiLayerPerceptron(nn.Module):\n","    # override constructor from nn.Module\n","    def __init__(self, num_inputs, num_hidden_layer_neurons=2):\n","        super().__init__() ## call constructor of nn.Module\n","\n","        # layer 1 defines the transformation from input to hidden layer\n","        self.layer1 = nn.Linear(input_dims=num_inputs, output_dims=num_hidden_layer_neurons)\n","        # layer 2 defines the transformation from hidden layer to output\n","        self.layer2 = nn.Linear(input_dims=num_hidden_layer_neurons, output_dims=1)\n","        self.sigmoid = mx.sigmoid\n","\n","    def __call__(self, x):\n","        return self.forward(x)\n","\n","    def forward(self, x):\n","        # x (input) -> hidden layer -> sigmoid -> output layer -> sigmoid\n","        x = self.sigmoid(self.layer1(x))\n","        x = self.sigmoid(self.layer2(x))\n","        return x        \n","\n","    "]},{"cell_type":"markdown","metadata":{},"source":["# Training with gradient descent"]},{"cell_type":"markdown","metadata":{},"source":["## Training and testing functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training function \n","def train(model, X, Y, optimizer, loss_and_grad_fn, num_epochs):\n","\n","    #  Loop over epochs\n","    for epoch in range(num_epochs):\n","\n","        # Reset accumulated loss per epoch\n","        acc_loss = 0\n","\n","        # Loop over all training data\n","        for i in range(len(X)):          \n","        \n","            # forward and backward pass\n","            loss, gradients = loss_and_grad_fn(model, X[i], Y[i].reshape(1,))     \n","            acc_loss += loss\n","\n","            # Update the model with the gradients. So far no computation has happened.\n","            optimizer.update(model, gradients)\n","\n","            # Compute the new parameters and also the new optimizer state.\n","            mx.eval(model.parameters(), optimizer.state)\n","        \n","\n","        # Print accumulated average loss per epoch once in a while\n","        if (epoch % (num_epochs//10)) == 0 or epoch == num_epochs - 1:     \n","            print(f\"Epoch {epoch:5d}: loss = {mx.mean(acc_loss).item():.5f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Testing function\n","\n","# The model will output values from 0-1.\n","# We can use a threshold of 0.5 to determine the class label.\n","\n","def test(model, X, Y):\n","    # test the model on all data points\n","    print(\"Testing ...\")\n","\n","    num_correct = 0\n","\n","    for i in range(len(X)): \n","        x = X[i]\n","        y = Y[i].reshape(1,)\n","\n","        prediction = model(x)\n","\n","        class_label = mx.where(prediction < 0.5, mx.array(0), mx.array(1))\n","        print(f\"{x} -> {class_label} ({prediction.item():.3f}) (label: {y})\")\n","\n","        if class_label == y:\n","            num_correct += 1\n","    \n","    # Print accuracy\n","    print(f\"Accuracy: {num_correct}/{val_size} = {100 * num_correct/val_size:.2f}%\")\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualization\n","\n","import matplotlib.cm as cm\n","import matplotlib.gridspec as gridspec\n","\n","\n","def show_decision_boundary(model, data, labels, subplot_spec=None):\n","\n","    data   = np.array(data)\n","    labels = np.array(labels)\n","\n","    wratio = (15, 1)\n","    if subplot_spec is None:\n","        gs = gridspec.GridSpec(1, 2, width_ratios=wratio)\n","    else:\n","        gs = gridspec.GridSpecFromSubplotSpec(1, 2, subplot_spec=subplot_spec, width_ratios=wratio)\n","        \n","    ax = plt.subplot(gs[0])\n","    ax.set_title('Dataset and Decision Function')\n","    \n","    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n","    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n","    h = 0.01  # Reduced step size for higher resolution\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","\n","    Z = model(mx.array(np.c_[xx.ravel(), yy.ravel()], dtype=mx.float32))\n","    Z = Z.reshape(xx.shape)\n","    \n","\n","    # Increase the number of levels for smoother color transitions\n","    levels = np.linspace(0, 1, 100)\n","    ctr = ax.contourf(xx, yy, np.array(Z), levels, cmap=cm.gray, vmin=0, vmax=1)\n","    \n","    unique_labels = np.unique(labels)\n","\n","    # Define colors for each class\n","    colors = ['red', 'blue']\n","    for i, yi in enumerate(unique_labels):\n","        color = colors[i]\n","        ax.scatter(data[np.where(labels.flatten() == yi), 0], data[np.where(labels.flatten() == yi), 1], \n","                   color=color, linewidth=0, label='Class %d (y=%d)' % (yi, yi))\n","    ax.legend()\n","    ax.set_xlim((x_min, x_max))\n","    ax.set_ylim((y_min, y_max))\n","\n","    # Create colorbar\n","    cbar = plt.colorbar(ctr, cax=plt.subplot(gs[1]))\n","    cbar.set_ticks(np.arange(0, 1.1, 0.1))  # Set ticks from 0 to 1 with 0.1 increments\n","    cbar.set_label('Decision value')"]},{"cell_type":"markdown","metadata":{},"source":["## Train and test the single perceptron"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train the perceptron model\n","\n","# The model to train\n","model = Perceptron(num_inputs=2)\n","# Evaluate because mlx uses lazy evaluation\n","mx.eval(model.parameters())\n","\n","# Hyperparameters\n","num_epochs = 50\n","eta = 0.01\n","\n","# Loss function\n","def loss_fn(model, X, y):  \n","    return nn.losses.mse_loss((model(X)), y)\n","\n","# Create the gradient function\n","loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n","\n","# Stochastic gradient descent (SGD) optimizer\n","optimizer = optim.SGD(learning_rate=eta)\n","\n","# Train the model\n","train(model, train_data, train_labels, optimizer, loss_and_grad_fn, num_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Test the model\n","test(model, val_data, val_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize the decision boundary\n","show_decision_boundary(model, data, labels)"]},{"cell_type":"markdown","metadata":{},"source":["## Train and test the MLP"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train the MLP model\n","\n","# The model to train\n","model = MultiLayerPerceptron(num_inputs=2)\n","# Evaluate because mlx uses lazy evaluation\n","mx.eval(model.parameters())\n","\n","# Hyperparameters\n","num_epochs = 50\n","eta = 0.01\n","\n","# Loss function\n","def loss_fn(model, X, y):  \n","    return nn.losses.mse_loss((model(X)), y)\n","\n","# Create the gradient function\n","loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n","\n","# Stochastic gradient descent (SGD) optimizer\n","optimizer = optim.SGD(learning_rate=eta)\n","\n","# Train the model\n","train(model, train_data, train_labels, optimizer, loss_and_grad_fn, num_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Test the model\n","test(model, val_data, val_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize the decision boundary\n","show_decision_boundary(model, data, labels)"]}],"metadata":{"colab":{"collapsed_sections":["JcF1mpJo-taz"],"provenance":[]},"kernelspec":{"display_name":"mlx-m1-2023-12","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
