{"cells":[{"cell_type":"markdown","metadata":{"id":"epxcwtWj5yJs"},"source":["# Multi-Layer Perceptron for a toy two-class problem in PyTorch\n","\n","Markus Enzweiler, markus.enzweiler@hs-esslingen.de\n","\n","This is a demo used in a Computer Vision & Machine Learning lecture. Feel free to use and contribute."]},{"cell_type":"markdown","metadata":{},"source":["We build and train a multi-layer perceptron (MLP) for a two-class classification problem with a *single* neuron in its output layer. The MLP will output values from 0-1 and we can use a threshold of 0.5 to determine the class label.\n","\n","We will also train a single perceptron for comparison. \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["See https://pytorch.org/docs/stable/nn.html and in particular:\n","- https://pytorch.org/docs/stable/generated/torch.nn.Module.html  \n","- https://pytorch.org/docs/stable/generated/torch.nn.Linear.html "]},{"cell_type":"markdown","metadata":{"id":"IJsjl_l47q28"},"source":["## Setup\n","\n","Adapt `packagePath` to point to the directory containing this notebeook."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":236,"status":"ok","timestamp":1703326573912,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"1iHkPBml98YG"},"outputs":[],"source":["# Imports\n","import sys\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Additional imports\n","\n","# Repository Root\n","repo_root = os.path.abspath(os.path.join(\"..\", \"..\"))\n","# Add the repository root to the system path\n","sys.path.append(repo_root)\n","\n","# Package Imports\n","from nbutils import requirements as nb_reqs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2602,"status":"ok","timestamp":1703326576880,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"DY4880S378_F","outputId":"a8ce2d26-fe24-4aa2-c232-acc31b71d394"},"outputs":[],"source":["# Package Path\n","package_path = \"./\" # local\n","print(f\"Package path: {package_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6888,"status":"ok","timestamp":1703326583765,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"iaURjw5n6pLq","outputId":"5c40ee52-1fa3-44df-a94e-b81bec144100"},"outputs":[],"source":["# Additional requirements for this notebook\n","req_file = os.path.join(package_path, \"requirements.txt\")\n","nb_reqs.pip_install_reqs(req_file)    "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1703326583765,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"iRERDI8aAnzr"},"outputs":[],"source":["# Now we should be able to import the additional packages\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","\n","# Set the random seed for reproducibility\n","np.random.seed(42)\n","torch.manual_seed(42);\n"]},{"cell_type":"markdown","metadata":{"id":"aUACL9QoFTXg"},"source":["## Create the training and validation data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Number of samples per class\n","n_samples = 1000\n","\n","# Generate random data for class 1\n","class_0 = torch.cat([\n","    torch.randn(n_samples//4, 2) + torch.tensor([2, 0]),\n","    torch.randn(n_samples//2, 2) + torch.tensor([0, 2]),\n","    torch.randn(n_samples//4, 2) + torch.tensor([2, 5])\n","    ])\n","\n","\n","# Generate random data for class 2\n","class_1 = torch.randn(n_samples, 2) + torch.tensor([4, 3])\n","\n","# Labels for the classes\n","labels_0 = torch.zeros(n_samples, 1)\n","labels_1 = torch.ones(n_samples, 1)\n","\n","# Combine the data and labels\n","data   = torch.cat([class_0, class_1],   dim=0)\n","labels = torch.cat([labels_0, labels_1], dim=0)\n","\n","# Plotting the data\n","plt.scatter(class_0[:, 0], class_0[:, 1], color='red', label='Class 0')\n","plt.scatter(class_1[:, 0], class_1[:, 1], color='blue',  label='Class 1')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.title('Two-dimensional Dataset with Two Classes')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split the data into training and validation data sets\n","# and wrap them in a Dataloader\n","\n","from torch.utils.data import TensorDataset, DataLoader, random_split\n","\n","# Combine data and labels into a dataset\n","dataset = TensorDataset(data, labels)\n","\n","# Define the size of the training and validation sets\n","train_size = int(0.8 * len(dataset))    # 80% for training\n","val_size   = len(dataset) - train_size  # 20% for validation\n","\n","# Split the dataset into training and validation sets\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","# You can also create DataLoader for batching if needed\n","train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n","val_loader   = DataLoader(val_dataset,   batch_size=1, shuffle=True)\n","\n","# Now, train_loader and val_loader can be used in a training loop"]},{"cell_type":"markdown","metadata":{},"source":["# Define the Multi-Layer Perceptron (MLP) and a single Perceptron"]},{"cell_type":"markdown","metadata":{},"source":["## Single perceptron class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perceptron model\n","class Perceptron(nn.Module):\n","    def __init__(self, num_inputs):\n","        super().__init__()\n","        self.linear = nn.Linear(num_inputs, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def __call__(self, x):\n","        return self.forward(x)\n","\n","    def forward(self, x):\n","        return self.sigmoid(self.linear(x))"]},{"cell_type":"markdown","metadata":{},"source":["## MLP class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Multi-layer perceptron model\n","class MultiLayerPerceptron(nn.Module):\n","    def __init__(self, num_inputs, num_hidden_layer_neurons=2):\n","        super().__init__()\n","\n","        # layer 1 defines the transformation from input to hidden layer\n","        self.layer1 = nn.Linear(num_inputs, num_hidden_layer_neurons)\n","        # layer 2 defines the transformation from hidden layer to output\n","        self.layer2 = nn.Linear(num_hidden_layer_neurons, 1)\n","        # sigmoid activation function\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def __call__(self, x):\n","        return self.forward(x)\n","\n","    def forward(self, x):\n","        # x (input) -> hidden layer -> sigmoid -> output layer -> sigmoid\n","        x = self.sigmoid(self.layer1(x))\n","        x = self.sigmoid(self.layer2(x))\n","        return x\n","    "]},{"cell_type":"markdown","metadata":{},"source":["# Training with gradient descent"]},{"cell_type":"markdown","metadata":{},"source":["## Training and testing functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training function \n","def train(model, train_loader, optimizer, loss_fn, num_epochs):\n","    \n","    # Loop over epochs\n","    for epoch in range(num_epochs):\n","\n","        # reset accumulated loss\n","        acc_loss = 0.0\n","\n","        for data in train_loader:\n","    \n","            # training sample and label\n","            x,y = data\n","\n","            # forward pass\n","            y_hat = model(x)\n","\n","            # backward pass\n","            loss = loss_fn(y_hat, y)\n","            loss.backward()\n","            \n","            # accumulate loss\n","            acc_loss += loss\n","\n","            # Update weights and bias\n","            optimizer.step()\n","            optimizer.zero_grad()\n","        \n","\n","        # Print accumulated average loss per epoch once in a while\n","        if (epoch % (num_epochs//10)) == 0 or epoch == num_epochs - 1:     \n","            print(f\"Epoch {epoch:5d}: loss = {torch.mean(acc_loss):.5f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Testing function\n","\n","# The model will output values from 0-1.\n","# We can use a threshold of 0.5 to determine the class label.\n","\n","def test(model, val_loader):\n","    # test the model on all data points\n","    print(\"Testing ...\")\n","\n","    num_correct = 0\n","\n","    for data in val_loader:  \n","        x,y = data\n","        prediction = model(x)\n","        class_label = torch.where(prediction < 0.5, torch.tensor(0), torch.tensor(1))\n","        print(f\"{x} -> {class_label} ({prediction.item():.3f}) (label: {y})\")\n","\n","        if class_label == y:\n","            num_correct += 1\n","    \n","    # Print accuracy\n","    print(f\"Accuracy: {num_correct}/{val_size} = {100 * num_correct/val_size:.2f}%\")\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualization\n","\n","import matplotlib.cm as cm\n","import matplotlib.gridspec as gridspec\n","\n","\n","def show_decision_boundary(model, data, labels, subplot_spec=None):\n","\n","    data   = data.numpy()\n","    labels = labels.numpy()\n","\n","    wratio = (15, 1)\n","    if subplot_spec is None:\n","        gs = gridspec.GridSpec(1, 2, width_ratios=wratio)\n","    else:\n","        gs = gridspec.GridSpecFromSubplotSpec(1, 2, subplot_spec=subplot_spec, width_ratios=wratio)\n","        \n","    ax = plt.subplot(gs[0])\n","    ax.set_title('Dataset and decision function')\n","    \n","    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n","    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n","    h = 0.01  # Reduced step size for higher resolution\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","\n","    Z = model(torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32))\n","    Z = Z.reshape(xx.shape)\n","\n","    # Increase the number of levels for smoother color transitions\n","    levels = np.linspace(0, 1, 100)\n","    ctr = ax.contourf(xx, yy, Z.detach().numpy(), levels, cmap=cm.gray, vmin=0, vmax=1)\n","    \n","    unique_labels = np.unique(labels)\n","\n","    # Define colors for each class\n","    colors = ['red', 'blue']\n","    for i, yi in enumerate(unique_labels):\n","        color = colors[i]\n","        ax.scatter(data[np.where(labels.flatten() == yi), 0], data[np.where(labels.flatten() == yi), 1], \n","                   color=color, linewidth=0, label='Class %d (y=%d)' % (yi, yi))\n","    ax.legend()\n","    ax.set_xlim((x_min, x_max))\n","    ax.set_ylim((y_min, y_max))\n","\n","    # Create colorbar\n","    cbar = plt.colorbar(ctr, cax=plt.subplot(gs[1]))\n","    cbar.set_ticks(np.arange(0, 1.1, 0.1))  # Set ticks from 0 to 1 with 0.1 increments\n","    cbar.set_label('Decision value')"]},{"cell_type":"markdown","metadata":{},"source":["## Train and test the single perceptron"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train the perceptron model\n","\n","# The model to train\n","model = Perceptron(num_inputs=2)\n","\n","# Hyperparameters\n","num_epochs = 50\n","eta = 0.01\n","\n","# Stochastic gradient descent (SGD) optimizer\n","optimizer = torch.optim.SGD(model.parameters(), lr=eta)\n","\n","# We can use L2 (mean squared error) loss from PyTorch\n","loss_fn = nn.MSELoss()\n","\n","# Train the model\n","train(model, train_loader, optimizer, loss_fn, num_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Test the model\n","test(model, val_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize the decision boundary\n","show_decision_boundary(model, data, labels)"]},{"cell_type":"markdown","metadata":{},"source":["## Train and test the MLP"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train the MLP model\n","\n","# The model to train\n","model = MultiLayerPerceptron(num_inputs=2)\n","\n","# Hyperparameters\n","num_epochs = 50\n","eta = 0.01\n","\n","# Stochastic gradient descent (SGD) optimizer\n","optimizer = torch.optim.SGD(model.parameters(), lr=eta)\n","\n","# We can use L2 (mean squared error) loss from PyTorch\n","loss_fn = nn.MSELoss()\n","\n","# Train the model\n","train(model, train_loader, optimizer, loss_fn, num_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Test the model\n","test(model, val_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize the decision boundary\n","show_decision_boundary(model, data, labels)"]}],"metadata":{"colab":{"collapsed_sections":["JcF1mpJo-taz"],"provenance":[]},"kernelspec":{"display_name":"pytorch-m1-2023-10","language":"python","name":"pytorch-m1-2023-10"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
