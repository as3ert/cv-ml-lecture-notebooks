{"cells":[{"cell_type":"markdown","metadata":{"id":"epxcwtWj5yJs"},"source":["# Automatic differentiation in PyTorch\n","\n","Markus Enzweiler, markus.enzweiler@hs-esslingen.de\n","\n","This is a demo used in a Computer Vision & Machine Learning lecture. Feel free to use and contribute.\n"]},{"cell_type":"markdown","source":["## Setup\n","\n","Adapt `packagePath` to point to the directory containing this notebeook, e.g. Colab or local."],"metadata":{"id":"IJsjl_l47q28"}},{"cell_type":"code","source":["# Imports\n","import sys\n","import os"],"metadata":{"id":"1iHkPBml98YG","executionInfo":{"status":"ok","timestamp":1703326573912,"user_tz":-60,"elapsed":236,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["# Package Path\n","package_path = \"./\" # local\n","\n","# Colab specific stuff below.\n","# If you use Colab, change package_path to a path on your Google drive\n","\n","def check_for_colab():\n","  try:\n","      import google.colab\n","      return True\n","  except ImportError:\n","      return False\n","\n","# Running on Colab?\n","on_colab = check_for_colab()\n","\n","# Google drive mount point\n","gdrive_mnt = '/content/drive'\n","\n","# Mount Google Drive if on Colab\n","if on_colab:\n","  from google.colab import drive\n","  drive.mount(gdrive_mnt, force_remount=True)\n","  package_path = f\"{gdrive_mnt}/MyDrive/colab/lecture_examples/cv-ml-lecture-notebooks/autograd\"   # Colab\n","\n","\n","print(f\"Package path: {package_path}\")"],"metadata":{"id":"DY4880S378_F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703326576880,"user_tz":-60,"elapsed":2602,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"}},"outputId":"a8ce2d26-fe24-4aa2-c232-acc31b71d394"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Package path: /content/drive/MyDrive/colab/lecture_examples/cv-ml-lecture-notebooks/autograd\n"]}]},{"cell_type":"code","source":["# Install requirements in the current Jupyter kernel\n","req_file = os.path.join(package_path, \"requirements.txt\")\n","if os.path.exists(req_file):\n","    !{sys.executable} -m pip install -r {req_file}\n","else:\n","    print(f\"Requirements file not found: {req_file}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iaURjw5n6pLq","executionInfo":{"status":"ok","timestamp":1703326583765,"user_tz":-60,"elapsed":6888,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"}},"outputId":"5c40ee52-1fa3-44df-a94e-b81bec144100"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/colab/lecture_examples/cv-ml-lecture-notebooks/autograd/requirements.txt (line 1)) (1.23.5)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/colab/lecture_examples/cv-ml-lecture-notebooks/autograd/requirements.txt (line 2)) (2.1.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/colab/lecture_examples/cv-ml-lecture-notebooks/autograd/requirements.txt (line 2)) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/colab/lecture_examples/cv-ml-lecture-notebooks/autograd/requirements.txt (line 2)) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/colab/lecture_examples/cv-ml-lecture-notebooks/autograd/requirements.txt (line 2)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/colab/lecture_examples/cv-ml-lecture-notebooks/autograd/requirements.txt (line 2)) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/colab/lecture_examples/cv-ml-lecture-notebooks/autograd/requirements.txt (line 2)) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/colab/lecture_examples/cv-ml-lecture-notebooks/autograd/requirements.txt (line 2)) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/colab/lecture_examples/cv-ml-lecture-notebooks/autograd/requirements.txt (line 2)) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r /content/drive/MyDrive/colab/lecture_examples/cv-ml-lecture-notebooks/autograd/requirements.txt (line 2)) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r /content/drive/MyDrive/colab/lecture_examples/cv-ml-lecture-notebooks/autograd/requirements.txt (line 2)) (1.3.0)\n"]}]},{"cell_type":"code","source":["# Now we should be able to import the additional packages\n","import torch\n","import numpy as np"],"metadata":{"id":"iRERDI8aAnzr","executionInfo":{"status":"ok","timestamp":1703326583765,"user_tz":-60,"elapsed":12,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"}}},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":["## Autograd\n","\n","Autograd in PyTorch is a powerful tool for automatic differentiation, enabling the efficient computation of gradients in neural networks and other computational graphs. Here's a brief overview:\n","\n","1. **Graph Construction**: During the forward pass, PyTorch builds a computational graph. Nodes represent tensors, while edges correspond to functions (operations) that transform these tensors.\n","\n","2. **Enable Gradient Tracking**: By setting `requires_grad=True` for a tensor, you tell PyTorch to track all operations on it. This is crucial for gradient computation.\n","\n","3. **Backward Propagation**: In the backward pass, PyTorch computes gradients by traversing this graph from outputs to inputs. This is done using the chain rule of calculus.\n","\n","4. **Gradient Calculation**: The gradients are calculated by `torch.autograd.grad` or `.backward()` methods. For $y = f(x)$, PyTorch computes $ \\frac{\\partial y}{\\partial x} $ by backtracking through the graph.\n","\n","\n","This system allows for efficient and flexible gradient computations, which is essential for training neural networks using gradient-based optimization methods.\n","\n","See:\n","- https://pytorch.org/docs/stable/autograd.html\n","- https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"],"metadata":{"id":"_aI-pchqASeB"}},{"cell_type":"markdown","source":["### Autograd with scalar functions"],"metadata":{"id":"aUACL9QoFTXg"}},{"cell_type":"code","source":["# Torch has \"autograd\" to automatically compute gradients\n","# Let's try it out with simple functions first.\n","\n","# Define the function, x^2+3x+2\n","def f(x):\n","    return x**2 + 3*x + 2\n","\n","# Manual gradient w.r.t x\n","def f_grad(x):\n","    return 2*x + 3\n","\n","def autograd(func, x):\n","    # Initialize an empty list for gradients\n","    gradients = []\n","\n","    # Compute the gradient for each element in the tensor\n","    for xi in x:\n","        # Compute the function on the i-th element\n","        y = func(xi)\n","\n","        # Compute the gradient for the i-th element\n","        gradients.append(torch.autograd.grad(outputs=y, inputs=xi)[0])\n","\n","        # The torch.autograd.grad function is designed to compute gradients of scalar outputs with respect to inputs.\n","        # In our case, the function f(x) applied to x_tensor results in a vector (a tensor with multiple elements),\n","        # not a single scalar. Hence, torch.autograd.grad cannot directly compute the gradient for each element\n","        # of this vector. To resolve this, we loop over each element of x_tensor, treating each function evaluation\n","        # f(x[i]) as a scalar output, and compute its gradient individually. This way, we are effectively computing\n","        # the gradient of multiple scalar functions, each dependent on a single element of x_tensor.\n","\n","    return torch.stack(gradients)\n","\n","\n","\n","# Compute some function values and gradients\n","# make sure to set requires_grad=True to enable gradient tracking on the computational graph\n","x_tensor = torch.arange(-5, 5, 1, dtype=torch.float32, requires_grad=True)\n","\n","f_value    = f(x_tensor)\n","f_grad     = f_grad(x_tensor)\n","f_autograd = autograd(f, x_tensor)\n","\n","for i in range(len(x_tensor)):\n","    print(f\"x = {x_tensor[i].item():5.2f}: \"\n","          f\"f(x) = {f_value[i].item():5.2f}, \"\n","          f\"f_grad(x) = {f_grad[i].item():5.2f}, \"\n","          f\"autograd(x) = {f_autograd[i].item():5.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lgoO6NHFAlcW","executionInfo":{"status":"ok","timestamp":1703326583765,"user_tz":-60,"elapsed":11,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"}},"outputId":"acd5d89c-aab0-4392-decd-3740da08d524"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["x = -5.00: f(x) = 12.00, f_grad(x) = -7.00, autograd(x) = -7.00\n","x = -4.00: f(x) =  6.00, f_grad(x) = -5.00, autograd(x) = -5.00\n","x = -3.00: f(x) =  2.00, f_grad(x) = -3.00, autograd(x) = -3.00\n","x = -2.00: f(x) =  0.00, f_grad(x) = -1.00, autograd(x) = -1.00\n","x = -1.00: f(x) =  0.00, f_grad(x) =  1.00, autograd(x) =  1.00\n","x =  0.00: f(x) =  2.00, f_grad(x) =  3.00, autograd(x) =  3.00\n","x =  1.00: f(x) =  6.00, f_grad(x) =  5.00, autograd(x) =  5.00\n","x =  2.00: f(x) = 12.00, f_grad(x) =  7.00, autograd(x) =  7.00\n","x =  3.00: f(x) = 20.00, f_grad(x) =  9.00, autograd(x) =  9.00\n","x =  4.00: f(x) = 30.00, f_grad(x) = 11.00, autograd(x) = 11.00\n"]}]},{"cell_type":"markdown","source":["### Autograd with tensors"],"metadata":{"id":"Cv03sygVFXEQ"}},{"cell_type":"code","source":["# Define two tensors and track computations\n","t1 = torch.tensor([[1, 2, 3],\n","                   [4, 5, 6]], dtype=torch.float32, requires_grad=True)\n","\n","t2 = torch.tensor([[7, 8, 9],\n","                   [10, 11, 12]], dtype=torch.float32, requires_grad=True)\n","\n","# Initially, gradients for t1 are None since no operations have been performed\n","print(f\"t1.grad = {t1.grad}\")\n","\n","# grad_fn for t1 is None because it is not a result of an operation\n","# but directly created from data in the computational graph\n","print(f\"t1.grad_fn = {t1.grad_fn}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E4PBVyEiFYjV","executionInfo":{"status":"ok","timestamp":1703326583765,"user_tz":-60,"elapsed":9,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"}},"outputId":"75cf9c1f-9a7d-4b3d-e481-2aecf6dcebf5"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["t1.grad = None\n","t1.grad_fn = None\n"]}]},{"cell_type":"code","source":["# Perform element-wise multiplication of t1 and t2\n","t1_mul_t2 = t1 * t2\n","\n","# The resulting tensor t1_mul_t2 has grad_fn set to MulBackward0,\n","# indicating that it's a result of a multiplication operation\n","print(f\"t1_mul_t2 = {t1_mul_t2}\")\n","\n","# Gradients for t1 are still None because backward() has not been called yet\n","print(f\"t1.grad = {t1.grad}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K6XvVawsGuz7","executionInfo":{"status":"ok","timestamp":1703326583765,"user_tz":-60,"elapsed":7,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"}},"outputId":"2bc21f21-24de-488e-cbaa-02efe744fa3c"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["t1_mul_t2 = tensor([[ 7., 16., 27.],\n","        [40., 55., 72.]], grad_fn=<MulBackward0>)\n","t1.grad = None\n"]}]},{"cell_type":"markdown","source":["After `backward()`, `t1.grad` and `t2.grad` are populated.\n","\n","The gradient of each element of `t1` is equal to the corresponding element in `t2`, and vice versa. This is because the derivative of `t1[i] * t2[i]` w.r.t. `t1[i]` is `t2[i]`, and w.r.t. `t2[i]` is `t1[i]`."],"metadata":{"id":"ZZ56e4tALbuL"}},{"cell_type":"code","source":["# Compute gradients of the sum of all elements in t1_mul_t2 with respect to t1 and t2\n","t1_mul_t2.sum().backward()\n","\n","# After backward(), t1.grad and t2.grad are populated.\n","# The gradient at each element in t1 and t2 indicates the rate of change of the sum with respect to that element.\n","# For element-wise multiplication, the gradient at each element of t1 is equal to the corresponding element\n","# in t2 and vice versa. This is because the derivative of t1[i] * t2[i] w.r.t. t1[i] is t2[i],\n","# and w.r.t. t2[i] is t1[i].\n","\n","print(f\"t1.grad = {t1.grad}\")\n","print(f\"t2.grad = {t2.grad}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tFqFLFpHHO8l","executionInfo":{"status":"ok","timestamp":1703326583766,"user_tz":-60,"elapsed":7,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"}},"outputId":"14c57946-85fb-450f-d07d-e9874ccbeab4"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["t1.grad = tensor([[ 7.,  8.,  9.],\n","        [10., 11., 12.]])\n","t2.grad = tensor([[1., 2., 3.],\n","        [4., 5., 6.]])\n"]}]},{"cell_type":"markdown","source":["### Why `.sum()` is needed for `.backward()`:\n","\n","\n","- PyTorch's `.backward()` function computes gradients with respect to a scalar value. This is essential because gradients are conceptually the rate of change of a scalar value with respect to other variables. If you have a tensor with more than one element and wish to compute gradients with respect to its elements, you need to first reduce it to a scalar. When performing operations between tensors, like `t1 * t2`, the result is another tensor. To compute gradients with respect to the original tensors (`t1` and `t2`), a scalar value is needed for differentiation. The `.sum()` method achieves this by combining all elements of the resulting tensor into a single scalar.\n","\n","- When `.backward()` is called on the scalar result of `t1_mul_t2.sum()`, it activates the chain rule in reverse throughout the computational graph. It calculates the gradient of the scalar with respect to each element in the tensors involved in the computation (`t1` and `t2`), effectively propagating the gradients backwards.\n","- The gradients computed in this manner indicate how much each element of `t1` and `t2` would need to change to increase the scalar sum. This approach is frequently utilized in optimization problems, where the scalar often represents a loss function.\n","\n","**In summary**, `.sum()` is employed to convert the tensor resulting from `t1 * t2` into a scalar, enabling `.backward()` to compute gradients. This procedure is standard in many deep learning applications, particularly in the computation of loss functions, where errors are backpropagated from a single scalar value (the loss) to update model parameters."],"metadata":{"id":"HhTqXltLKgae"}},{"cell_type":"code","source":["# Analyzing the gradient at t2[0,1]. If t2.grad[0,1] is 2, it means that a unit change in t2[0,1] results in a\n","# change of 2 in the sum. Therefore, increasing t2[0,1] by 3 should increase the sum by 3 * t2.grad[0,1], under\n","# linear approximation.\n","\n","# Create a new tensor and add 3 to t2[0,1]\n","t2_modified = t2.clone()\n","t2_modified[0,1] = t2[0,1] + 3\n","\n","# Perform the computation again with the modified t2\n","t1_mul_t2_updated = t1 * t2_modified\n","updated_sum = t1_mul_t2_updated.sum()\n","\n","# Compare the change in sum\n","change_in_sum = updated_sum - t1_mul_t2.sum()\n","print(f\"Change in sum: {change_in_sum}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mceG39AiMCuM","executionInfo":{"status":"ok","timestamp":1703327263018,"user_tz":-60,"elapsed":244,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"}},"outputId":"d40802e9-5ee5-4fe8-e694-4f9ad3e6b045"},"execution_count":83,"outputs":[{"output_type":"stream","name":"stdout","text":["Change in sum: 6.0\n"]}]},{"cell_type":"markdown","source":["## Finish"],"metadata":{"id":"JcF1mpJo-taz"}},{"cell_type":"code","source":["if on_colab and os.path.exists('/content/drive'):\n","  drive.flush_and_unmount()"],"metadata":{"id":"D_mLlkzk-vel","executionInfo":{"status":"ok","timestamp":1703327280900,"user_tz":-60,"elapsed":268,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"}}},"execution_count":84,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"colab":{"provenance":[],"collapsed_sections":["JcF1mpJo-taz"]}},"nbformat":4,"nbformat_minor":0}