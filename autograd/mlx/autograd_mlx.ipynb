{"cells":[{"cell_type":"markdown","metadata":{"id":"epxcwtWj5yJs"},"source":["# Automatic differentiation in MLX\n","\n","Markus Enzweiler, markus.enzweiler@hs-esslingen.de\n","\n","This is a demo used in a Computer Vision & Machine Learning lecture. Feel free to use and contribute.\n","\n","**Note: This requires a machine with an Apple SoC, e.g. M1/M2/M3 etc.**\n","\n","See: https://github.com/ml-explore/mlx\n"]},{"cell_type":"markdown","metadata":{"id":"IJsjl_l47q28"},"source":["## Setup\n","\n","Adapt `packagePath` to point to the directory containing this notebeook."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":236,"status":"ok","timestamp":1703326573912,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"1iHkPBml98YG"},"outputs":[],"source":["# Imports\n","import sys\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2602,"status":"ok","timestamp":1703326576880,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"DY4880S378_F","outputId":"a8ce2d26-fe24-4aa2-c232-acc31b71d394"},"outputs":[],"source":["# Package Path\n","package_path = \"./\" # local\n","print(f\"Package path: {package_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Additional imports\n","\n","# Repository Root\n","repo_root = os.path.abspath(os.path.join(\"..\", \"..\"))\n","# Add the repository root to the system path\n","sys.path.append(repo_root)\n","\n","# Package Imports\n","from nbutils import requirements as reqs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6888,"status":"ok","timestamp":1703326583765,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"iaURjw5n6pLq","outputId":"5c40ee52-1fa3-44df-a94e-b81bec144100"},"outputs":[],"source":["# Install requirements from requirements.txt\n","req_file = os.path.join(package_path, \"requirements.txt\")\n","reqs.pip_install_reqs(req_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1703326583765,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"iRERDI8aAnzr"},"outputs":[],"source":["# Now we should be able to import the additional packages\n","import mlx \n","import mlx.core as mx"]},{"cell_type":"markdown","metadata":{"id":"_aI-pchqASeB"},"source":["## MLX\n","\n","MLX provides composable function transformations, supporting automatic differentiation, automatic vectorization, and optimization of computation graphs. Computation graphs within MLX are dynamically constructed. A key feature of MLX is the use of (and optimization for) unified memory present in the Apple SoCs. \n","\n","See:\n","- https://github.com/ml-explore/mlx\n","- https://ml-explore.github.io/mlx/build/html/quick_start.html"]},{"cell_type":"markdown","metadata":{"id":"aUACL9QoFTXg"},"source":["### Automatic differentiation with scalar functions in MLX"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def f(x):\n","    return x**2 + 3*x + 2\n","\n","\n","x_tensor = mx.arange(-5, 5, 1, dtype=mx.float32)\n","print(mx.grad(f)(x_tensor[0]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1703326583765,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"lgoO6NHFAlcW","outputId":"acd5d89c-aab0-4392-decd-3740da08d524"},"outputs":[],"source":["# MLX has mlx.core.grad to automatically compute gradients of a function. \n","# See: https://ml-explore.github.io/mlx/build/html/python/_autosummary/mlx.core.grad.html#mlx.core.grad\n","\n","# Let's try it out with simple functions first.\n","\n","# Define the function, x^2+3x+2\n","def f(x):\n","    return x**2 + 3*x + 2\n","\n","# Manual gradient w.r.t x\n","def f_grad(x):\n","    return 2*x + 3\n","\n","def mlxgrad(func, x):\n","    # Initialize an empty list for gradients\n","    gradients = []\n","\n","    # Compute the gradient for each element in the tensor\n","    for xi in x:\n","        # Compute the function on the i-th element\n","        y = func(xi)\n","\n","        # Compute the gradient for the i-th element\n","        gradients.append(mlx.core.grad(func)(xi))\n","            \n","        # Similar to PyTorch autograd, the mlx.core.grad function is designed to compute gradients of scalar \n","        # outputs with respect to inputs.\n","        \n","        # In our case, the function f(x) applied to x_tensor results in a vector (a tensor with multiple elements),\n","        # not a single scalar. Hence, mlx.core.grad cannot directly compute the gradient for each element\n","        # of this vector. To resolve this, we loop over each element of x_tensor, treating each function evaluation\n","        # f(x[i]) as a scalar output, and compute its gradient individually. This way, we are effectively computing\n","        # the gradient of multiple scalar functions, each dependent on a single element of x_tensor.\n","\n","    return gradients\n","\n","\n","\n","# Compute some function values and gradients\n","# make sure to set requires_grad=True to enable gradient tracking on the computational graph\n","x_tensor = mx.arange(-5, 5, 1, dtype=mx.float32)\n","\n","f_value    = f(x_tensor)\n","f_grad     = f_grad(x_tensor)\n","f_autograd = mlxgrad(f, x_tensor)\n","\n","for i in range(len(x_tensor)):\n","    print(f\"x = {x_tensor[i].item():5.2f}: \"\n","          f\"f(x) = {f_value[i].item():5.2f}, \"\n","          f\"f_grad(x) = {f_grad[i].item():5.2f}, \"\n","          f\"autograd(x) = {f_autograd[i].item():5.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"Cv03sygVFXEQ"},"source":["### Automatic differentiation with tensors in MLX"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1703326583765,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"E4PBVyEiFYjV","outputId":"75cf9c1f-9a7d-4b3d-e481-2aecf6dcebf5"},"outputs":[],"source":["# Define two tensors and track computations\n","t1 = mx.array([[1, 2, 3],\n","               [4, 5, 6]], dtype=mx.float32)\n","\n","t2 = mx.array([[7, 8, 9],\n","              [10, 11, 12]], dtype=mx.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1703326583765,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"K6XvVawsGuz7","outputId":"2bc21f21-24de-488e-cbaa-02efe744fa3c"},"outputs":[],"source":["# Perform element-wise multiplication of t1 and t2\n","t1_mul_t2 = t1 * t2\n","\n","def mul_and_reduce_sum(t1, t2):\n","    return mx.sum(t1*t2)"]},{"cell_type":"markdown","metadata":{"id":"ZZ56e4tALbuL"},"source":["After `backward()`, `t1.grad` and `t2.grad` are populated.\n","\n","The gradient of each element of `t1` is equal to the corresponding element in `t2`, and vice versa. This is because the derivative of `t1[i] * t2[i]` w.r.t. `t1[i]` is `t2[i]`, and w.r.t. `t2[i]` is `t1[i]`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1703326583766,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"tFqFLFpHHO8l","outputId":"14c57946-85fb-450f-d07d-e9874ccbeab4"},"outputs":[],"source":["# Compute gradients of the sum of all elements in t1_mul_t2 with respect to t1 and t2\n","\n","lvalue, t1_grad = mx.value_and_grad(mul_and_reduce_sum)(t1,t2)\n","lvalue, t2_grad = mx.value_and_grad(mul_and_reduce_sum)(t2,t1)\n","\n","\n","# The gradient at each element in t1 and t2 indicates the rate of change of the sum with respect to that element.\n","# For element-wise multiplication, the gradient at each element of t1 is equal to the corresponding element\n","# in t2 and vice versa. This is because the derivative of t1[i] * t2[i] w.r.t. t1[i] is t2[i],\n","# and w.r.t. t2[i] is t1[i].\n","\n","print(f\"t1_grad = {t1_grad}\")\n","print(f\"t2_grad = {t2_grad}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":244,"status":"ok","timestamp":1703327263018,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"mceG39AiMCuM","outputId":"d40802e9-5ee5-4fe8-e694-4f9ad3e6b045"},"outputs":[],"source":["# Analyzing the gradient at t2[0,1]. If t2_grad[0,1] is 2, it means that a unit change in t2[0,1] results in a\n","# change of 2 in the sum. Therefore, increasing t2[0,1] by 3 should increase the sum by 3 * t2_grad[0,1], under\n","# linear approximation.\n","\n","# Create a new tensor and add 3 to t2[0,1]\n","t2_modified = t2\n","t2_modified[0,1] = t2[0,1] + 3\n","\n","# Perform the computation again with the modified t2\n","t1_mul_t2_updated = t1 * t2_modified\n","updated_sum = t1_mul_t2_updated.sum()\n","\n","# Compare the change in sum\n","change_in_sum = updated_sum - t1_mul_t2.sum()\n","print(f\"Change in sum: {change_in_sum}\")"]}],"metadata":{"colab":{"collapsed_sections":["JcF1mpJo-taz"],"provenance":[]},"kernelspec":{"display_name":"mlx-m1-2023-12","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
