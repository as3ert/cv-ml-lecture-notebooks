{"cells":[{"cell_type":"markdown","metadata":{"id":"epxcwtWj5yJs"},"source":["# Automatic differentiation in PyTorch\n","\n","Markus Enzweiler, markus.enzweiler@hs-esslingen.de\n","\n","This is a demo used in a Computer Vision & Machine Learning lecture. Feel free to use and contribute.\n"]},{"cell_type":"markdown","metadata":{"id":"IJsjl_l47q28"},"source":["## Setup\n","\n","Adapt `packagePath` to point to the directory containing this notebeook, e.g. Colab or local."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":236,"status":"ok","timestamp":1703326573912,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"1iHkPBml98YG"},"outputs":[],"source":["# Imports\n","import sys\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2602,"status":"ok","timestamp":1703326576880,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"DY4880S378_F","outputId":"a8ce2d26-fe24-4aa2-c232-acc31b71d394"},"outputs":[],"source":["# Package Path\n","package_path = \"./\" # local\n","print(f\"Package path: {package_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Additional imports\n","\n","# Repository Root\n","repo_root = os.path.abspath(os.path.join(\"..\", \"..\"))\n","# Add the repository root to the system path\n","sys.path.append(repo_root)\n","\n","# Package Imports\n","from nbutils import requirements as reqs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6888,"status":"ok","timestamp":1703326583765,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"iaURjw5n6pLq","outputId":"5c40ee52-1fa3-44df-a94e-b81bec144100"},"outputs":[],"source":["# Install requirements from requirements.txt\n","req_file = os.path.join(package_path, \"requirements.txt\")\n","reqs.pip_install_reqs(req_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1703326583765,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"iRERDI8aAnzr"},"outputs":[],"source":["# Now we should be able to import the additional packages\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"_aI-pchqASeB"},"source":["## Autograd\n","\n","Autograd in PyTorch is a powerful tool for automatic differentiation, enabling the efficient computation of gradients in neural networks and other computational graphs. Here's a brief overview:\n","\n","1. **Graph Construction**: During the forward pass, PyTorch builds a computational graph. Nodes represent tensors, while edges correspond to functions (operations) that transform these tensors.\n","\n","2. **Enable Gradient Tracking**: By setting `requires_grad=True` for a tensor, you tell PyTorch to track all operations on it. This is crucial for gradient computation.\n","\n","3. **Backward Propagation**: In the backward pass, PyTorch computes gradients by traversing this graph from outputs to inputs. This is done using the chain rule of calculus.\n","\n","4. **Gradient Calculation**: The gradients are calculated by `torch.autograd.grad` or `.backward()` methods. For $y = f(x)$, PyTorch computes $ \\frac{\\partial y}{\\partial x} $ by backtracking through the graph.\n","\n","\n","This system allows for efficient and flexible gradient computations, which is essential for training neural networks using gradient-based optimization methods.\n","\n","See:\n","- https://pytorch.org/docs/stable/autograd.html\n","- https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"]},{"cell_type":"markdown","metadata":{"id":"aUACL9QoFTXg"},"source":["### Autograd with scalar functions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1703326583765,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"lgoO6NHFAlcW","outputId":"acd5d89c-aab0-4392-decd-3740da08d524"},"outputs":[],"source":["# Torch has \"autograd\" to automatically compute gradients\n","# Let's try it out with simple functions first.\n","\n","# Define the function, x^2+3x+2\n","def f(x):\n","    return x**2 + 3*x + 2\n","\n","# Manual gradient w.r.t x\n","def f_grad(x):\n","    return 2*x + 3\n","\n","def autograd(func, x):\n","    # Initialize an empty list for gradients\n","    gradients = []\n","\n","    # Compute the gradient for each element in the tensor\n","    for xi in x:\n","        # Compute the function on the i-th element\n","        y = func(xi)\n","\n","        # Compute the gradient for the i-th element\n","        gradients.append(torch.autograd.grad(outputs=y, inputs=xi)[0])\n","\n","        # The torch.autograd.grad function is designed to compute gradients of scalar outputs with respect to inputs.\n","        # In our case, the function f(x) applied to x_tensor results in a vector (a tensor with multiple elements),\n","        # not a single scalar. Hence, torch.autograd.grad cannot directly compute the gradient for each element\n","        # of this vector. To resolve this, we loop over each element of x_tensor, treating each function evaluation\n","        # f(x[i]) as a scalar output, and compute its gradient individually. This way, we are effectively computing\n","        # the gradient of multiple scalar functions, each dependent on a single element of x_tensor.\n","\n","    return torch.stack(gradients)\n","\n","\n","\n","# Compute some function values and gradients\n","# make sure to set requires_grad=True to enable gradient tracking on the computational graph\n","x_tensor = torch.arange(-5, 5, 1, dtype=torch.float32, requires_grad=True)\n","\n","f_value    = f(x_tensor)\n","f_grad     = f_grad(x_tensor)\n","f_autograd = autograd(f, x_tensor)\n","\n","for i in range(len(x_tensor)):\n","    print(f\"x = {x_tensor[i].item():5.2f}: \"\n","          f\"f(x) = {f_value[i].item():5.2f}, \"\n","          f\"f_grad(x) = {f_grad[i].item():5.2f}, \"\n","          f\"autograd(x) = {f_autograd[i].item():5.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"Cv03sygVFXEQ"},"source":["### Autograd with tensors"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1703326583765,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"E4PBVyEiFYjV","outputId":"75cf9c1f-9a7d-4b3d-e481-2aecf6dcebf5"},"outputs":[],"source":["# Define two tensors and track computations\n","t1 = torch.tensor([[1, 2, 3],\n","                   [4, 5, 6]], dtype=torch.float32, requires_grad=True)\n","\n","t2 = torch.tensor([[7, 8, 9],\n","                   [10, 11, 12]], dtype=torch.float32, requires_grad=True)\n","\n","# Initially, gradients for t1 are None since no operations have been performed\n","print(f\"t1.grad = {t1.grad}\")\n","\n","# grad_fn for t1 is None because it is not a result of an operation\n","# but directly created from data in the computational graph\n","print(f\"t1.grad_fn = {t1.grad_fn}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1703326583765,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"K6XvVawsGuz7","outputId":"2bc21f21-24de-488e-cbaa-02efe744fa3c"},"outputs":[],"source":["# Perform element-wise multiplication of t1 and t2\n","t1_mul_t2 = t1 * t2\n","\n","# The resulting tensor t1_mul_t2 has grad_fn set to MulBackward0,\n","# indicating that it's a result of a multiplication operation\n","print(f\"t1_mul_t2 = {t1_mul_t2}\")\n","\n","# Gradients for t1 are still None because backward() has not been called yet\n","print(f\"t1.grad = {t1.grad}\")"]},{"cell_type":"markdown","metadata":{"id":"ZZ56e4tALbuL"},"source":["After `backward()`, `t1.grad` and `t2.grad` are populated.\n","\n","The gradient of each element of `t1` is equal to the corresponding element in `t2`, and vice versa. This is because the derivative of `t1[i] * t2[i]` w.r.t. `t1[i]` is `t2[i]`, and w.r.t. `t2[i]` is `t1[i]`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1703326583766,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"tFqFLFpHHO8l","outputId":"14c57946-85fb-450f-d07d-e9874ccbeab4"},"outputs":[],"source":["# Compute gradients of the sum of all elements in t1_mul_t2 with respect to t1 and t2\n","t1_mul_t2.sum().backward()\n","\n","# After backward(), t1.grad and t2.grad are populated.\n","# The gradient at each element in t1 and t2 indicates the rate of change of the sum with respect to that element.\n","# For element-wise multiplication, the gradient at each element of t1 is equal to the corresponding element\n","# in t2 and vice versa. This is because the derivative of t1[i] * t2[i] w.r.t. t1[i] is t2[i],\n","# and w.r.t. t2[i] is t1[i].\n","\n","print(f\"t1.grad = {t1.grad}\")\n","print(f\"t2.grad = {t2.grad}\")"]},{"cell_type":"markdown","metadata":{"id":"HhTqXltLKgae"},"source":["### Why `.sum()` is needed for `.backward()`:\n","\n","\n","- PyTorch's `.backward()` function computes gradients with respect to a scalar value. This is essential because gradients are conceptually the rate of change of a scalar value with respect to other variables. If you have a tensor with more than one element and wish to compute gradients with respect to its elements, you need to first reduce it to a scalar. When performing operations between tensors, like `t1 * t2`, the result is another tensor. To compute gradients with respect to the original tensors (`t1` and `t2`), a scalar value is needed for differentiation. The `.sum()` method achieves this by combining all elements of the resulting tensor into a single scalar.\n","\n","- When `.backward()` is called on the scalar result of `t1_mul_t2.sum()`, it activates the chain rule in reverse throughout the computational graph. It calculates the gradient of the scalar with respect to each element in the tensors involved in the computation (`t1` and `t2`), effectively propagating the gradients backwards.\n","- The gradients computed in this manner indicate how much each element of `t1` and `t2` would need to change to increase the scalar sum. This approach is frequently utilized in optimization problems, where the scalar often represents a loss function.\n","\n","**In summary**, `.sum()` is employed to convert the tensor resulting from `t1 * t2` into a scalar, enabling `.backward()` to compute gradients. This procedure is standard in many deep learning applications, particularly in the computation of loss functions, where errors are backpropagated from a single scalar value (the loss) to update model parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":244,"status":"ok","timestamp":1703327263018,"user":{"displayName":"Markus Enzweiler","userId":"04524044579212347608"},"user_tz":-60},"id":"mceG39AiMCuM","outputId":"d40802e9-5ee5-4fe8-e694-4f9ad3e6b045"},"outputs":[],"source":["# Analyzing the gradient at t2[0,1]. If t2.grad[0,1] is 2, it means that a unit change in t2[0,1] results in a\n","# change of 2 in the sum. Therefore, increasing t2[0,1] by 3 should increase the sum by 3 * t2.grad[0,1], under\n","# linear approximation.\n","\n","# Create a new tensor and add 3 to t2[0,1]\n","t2_modified = t2.clone()\n","t2_modified[0,1] = t2[0,1] + 3\n","\n","# Perform the computation again with the modified t2\n","t1_mul_t2_updated = t1 * t2_modified\n","updated_sum = t1_mul_t2_updated.sum()\n","\n","# Compare the change in sum\n","change_in_sum = updated_sum - t1_mul_t2.sum()\n","print(f\"Change in sum: {change_in_sum}\")"]}],"metadata":{"colab":{"collapsed_sections":["JcF1mpJo-taz"],"provenance":[]},"kernelspec":{"display_name":"pytorch-m1-2023-10","language":"python","name":"pytorch-m1-2023-10"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
